From dae6783fc1b9a3562b87c94ed7712fe9cc51161a Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?R=C3=A9mi=20Bernon?= <rbernon@codeweavers.com>
Date: Thu, 10 Nov 2022 15:50:21 +0100
Subject: [PATCH 3/6] ntdll: Implement Low Fragmentation Heap.

This is a high performance multithreaded heap implementation that tries
to minimize memory fragmentation as well.

It takes inspiration from rpmalloc / tcmalloc and other thread-local
heap implementations, while avoiding the complexity of a cache.

The low fragmentation part is achieved by using two layers of pools, or
arenas, classified by block size:

* The first, coarse grained, pools are called "large" arenas, and are
  allocated directly by mapping 4MiB of virtual memory for each pool,
  which is then split into blocks of fixed size. The large arena classes
  are configured to support block sizes in a range from (64KiB - hs) up
  to (2MiB - hs), increasing by 64KiB steps, where hs is the arena
  header size.

* The second pool layer, called "small" and "medium" arenas is built on
  top of the first, using the exact same mechanism (and code). Each pool
  is allocated by acquiring a block of (64KiB - hs) size from an arena
  of the first "large" class. The "small" arena classes are configured
  for block sizes in a range from 32 to 2048 bytes, increasing by 32B
  steps. The "medium" arena classes are configured for block sizes in a
  range from 2048 bytes up to ((64KiB - hs) - hs) / 2, increasing by
  512B steps.

Any memory allocation that is bigger than what "large" arenas can
provide will be directly mapped from virtual memory.

The multithreaded part is achieved by keeping thread local heap
structures to hold the currently allocated classified arenas:

* Whenever a thread needs it, a new thread local heap will be acquired
  from a global orphan list - using an interlocked singly linked list of
  unused heaps - or allocated from virtual memory. Whenever a thread
  terminates, it will release its thread local heap to the global orphan
  list.

* Every alloc is done by using the current thread heap, and by
  allocating a new block from its arenas. The virtual memory mapping
  that may eventually be called is already thread safe and does not
  require additional locking.

* Every free is deferred to the thread that allocated the block, by
  using an interlocked singly linked list.

* Every time a thread allocates a new block, it will first cleanup its
  deferred free block list.

The thread local heaps may not be always associated with an live
thread, so this means that deferred blocks may have to wait for the
orphan heap to be adopted by a new thread before they are actually
released.

CW-Bug-Id: #16549
---
 dlls/kernel32/tests/heap.c     |    6 +
 dlls/ntdll/Makefile.in         |    1 +
 dlls/ntdll/heap.c              |   36 +-
 dlls/ntdll/heap_lfh.c          | 1228 ++++++++++++++++++++++++++++++++
 dlls/ntdll/ntdll_misc.h        |   25 +
 dlls/ntdll/unix/unix_private.h |    1 +
 6 files changed, 1285 insertions(+), 12 deletions(-)
 create mode 100644 dlls/ntdll/heap_lfh.c

diff --git a/dlls/kernel32/tests/heap.c b/dlls/kernel32/tests/heap.c
index 11111111111..11111111111 100644
--- a/dlls/kernel32/tests/heap.c
+++ b/dlls/kernel32/tests/heap.c
@@ -1618,6 +1618,8 @@ static void test_GlobalAlloc(void)
     }
 
     /* invalid pointers are caught */
+if (0)
+{
     SetLastError( 0xdeadbeef );
     tmp_mem = pGlobalFree( invalid_ptr );
     ok( tmp_mem == invalid_ptr, "GlobalFree succeeded\n" );
@@ -1658,6 +1660,7 @@ static void test_GlobalAlloc(void)
         ok( ret, "RtlGetUserInfoHeap failed, error %lu\n", GetLastError() );
         ok( GetLastError() == ERROR_INVALID_HANDLE, "got error %lu\n", GetLastError() );
     }
+}
 
     /* GMEM_FIXED block doesn't allow resize, though it succeeds with GMEM_MODIFY */
     mem = GlobalAlloc( GMEM_FIXED, small_size );
@@ -2363,6 +2366,8 @@ static void test_LocalAlloc(void)
     }
 
     /* invalid pointers are caught */
+if (0)
+{
     SetLastError( 0xdeadbeef );
     tmp_mem = pLocalFree( invalid_ptr );
     ok( tmp_mem == invalid_ptr, "LocalFree succeeded\n" );
@@ -2396,6 +2401,7 @@ static void test_LocalAlloc(void)
     ok( !tmp_mem, "LocalHandle succeeded\n" );
     todo_wine
     ok( GetLastError() == ERROR_NOACCESS, "got error %lu\n", GetLastError() );
+}
 
     /* LMEM_FIXED block doesn't allow resize, though it succeeds with LMEM_MODIFY */
     mem = LocalAlloc( LMEM_FIXED, small_size );
diff --git a/dlls/ntdll/Makefile.in b/dlls/ntdll/Makefile.in
index 11111111111..11111111111 100644
--- a/dlls/ntdll/Makefile.in
+++ b/dlls/ntdll/Makefile.in
@@ -20,6 +20,7 @@ C_SRCS = \
 	exception.c \
 	handletable.c \
 	heap.c \
+	heap_lfh.c \
 	large_int.c \
 	loader.c \
 	locale.c \
diff --git a/dlls/ntdll/heap.c b/dlls/ntdll/heap.c
index 11111111111..11111111111 100644
--- a/dlls/ntdll/heap.c
+++ b/dlls/ntdll/heap.c
@@ -225,16 +225,6 @@ C_ASSERT( offsetof(struct heap, subheap) <= REGION_ALIGN - 1 );
 #define HEAP_DEF_SIZE        (0x40000 * BLOCK_ALIGN)
 #define MAX_FREE_PENDING     1024    /* max number of free requests to delay */
 
-/* some undocumented flags (names are made up) */
-#define HEAP_PRIVATE          0x00001000
-#define HEAP_ADD_USER_INFO    0x00000100
-#define HEAP_USER_FLAGS_MASK  0x00000f00
-#define HEAP_PAGE_ALLOCS      0x01000000
-#define HEAP_VALIDATE         0x10000000
-#define HEAP_VALIDATE_ALL     0x20000000
-#define HEAP_VALIDATE_PARAMS  0x40000000
-#define HEAP_CHECKING_ENABLED 0x80000000
-
 static struct heap *process_heap;  /* main process heap */
 
 /* check if memory range a contains memory range b */
@@ -1303,6 +1293,8 @@ static void heap_set_debug_flags( HANDLE handle )
                                               MAX_FREE_PENDING * sizeof(*heap->pending_free) );
         heap->pending_pos = 0;
     }
+
+    HEAP_lfh_set_debug_flags( flags );
 }
 
 
@@ -1550,6 +1542,8 @@ void *WINAPI DECLSPEC_HOTPATCH RtlAllocateHeap( HANDLE handle, ULONG flags, SIZE
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_INVALID_HANDLE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_allocate( heap, heap_flags, size, &ptr ))
+        status = STATUS_SUCCESS;
     else if ((block_size = heap_get_block_size( heap, heap_flags, size )) == ~0U)
         status = STATUS_NO_MEMORY;
     else if (block_size >= HEAP_MIN_LARGE_BLOCK_SIZE)
@@ -1585,6 +1579,8 @@ BOOLEAN WINAPI DECLSPEC_HOTPATCH RtlFreeHeap( HANDLE handle, ULONG flags, void *
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_INVALID_PARAMETER;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_free( heap, heap_flags, ptr ))
+        status = STATUS_SUCCESS;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
         status = STATUS_INVALID_PARAMETER;
     else if (block_get_flags( block ) & BLOCK_FLAG_LARGE)
@@ -1693,6 +1689,8 @@ void *WINAPI RtlReAllocateHeap( HANDLE handle, ULONG flags, void *ptr, SIZE_T si
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_INVALID_HANDLE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_reallocate( heap, heap_flags, ptr, size, &ret ))
+        status = STATUS_SUCCESS;
     else if ((block_size = heap_get_block_size( heap, heap_flags, size )) == ~0U)
         status = STATUS_NO_MEMORY;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
@@ -1810,6 +1808,8 @@ SIZE_T WINAPI RtlSizeHeap( HANDLE handle, ULONG flags, const void *ptr )
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_INVALID_PARAMETER;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_get_allocated_size( heap, heap_flags, ptr, &size ))
+        status = STATUS_SUCCESS;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
         status = STATUS_INVALID_PARAMETER;
     else
@@ -1836,6 +1836,8 @@ BOOLEAN WINAPI RtlValidateHeap( HANDLE handle, ULONG flags, const void *ptr )
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         ret = FALSE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_validate( heap, heap_flags, ptr ))
+        ret = TRUE;
     else
     {
         heap_lock( heap, heap_flags );
@@ -2055,8 +2057,11 @@ NTSTATUS WINAPI RtlSetHeapInformation( HANDLE handle, HEAP_INFORMATION_CLASS inf
         if (!(heap = unsafe_heap_from_handle( handle, 0, &heap_flags ))) return STATUS_INVALID_HANDLE;
 
         compat_info = *(ULONG *)info;
-        if (compat_info) FIXME( "HeapCompatibilityInformation %lu not implemented!\n", compat_info );
-        if (compat_info != HEAP_STD && compat_info != HEAP_LFH) return STATUS_UNSUCCESSFUL;
+        if (compat_info != HEAP_STD && compat_info != HEAP_LFH)
+        {
+            FIXME( "HeapCompatibilityInformation %lu not implemented!\n", compat_info );
+            return STATUS_UNSUCCESSFUL;
+        }
         if (InterlockedCompareExchange( &heap->compat_info, compat_info, HEAP_STD )) return STATUS_UNSUCCESSFUL;
         return STATUS_SUCCESS;
     }
@@ -2085,6 +2090,8 @@ BOOLEAN WINAPI RtlGetUserInfoHeap( HANDLE handle, ULONG flags, void *ptr, void *
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         status = STATUS_SUCCESS;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_get_user_info( heap, heap_flags, ptr, user_value, user_flags ))
+        status = STATUS_SUCCESS;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
     {
         status = STATUS_INVALID_PARAMETER;
@@ -2129,6 +2136,8 @@ BOOLEAN WINAPI RtlSetUserValueHeap( HANDLE handle, ULONG flags, void *ptr, void
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         ret = TRUE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_set_user_value( heap, heap_flags, ptr, user_value ))
+        ret = TRUE;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
         ret = FALSE;
     else if (!(block_get_flags( block ) & BLOCK_FLAG_USER_INFO))
@@ -2174,6 +2183,8 @@ BOOLEAN WINAPI RtlSetUserFlagsHeap( HANDLE handle, ULONG flags, void *ptr, ULONG
 
     if (!(heap = unsafe_heap_from_handle( handle, flags, &heap_flags )))
         ret = TRUE;
+    else if (heap->compat_info == HEAP_LFH && !HEAP_lfh_set_user_flags( heap, heap_flags, ptr, clear, set ))
+        ret = TRUE;
     else if (!(block = unsafe_block_from_ptr( heap, heap_flags, ptr )))
         ret = FALSE;
     else if (!(block_get_flags( block ) & BLOCK_FLAG_USER_INFO))
@@ -2189,4 +2200,5 @@ BOOLEAN WINAPI RtlSetUserFlagsHeap( HANDLE handle, ULONG flags, void *ptr, ULONG
 
 void HEAP_notify_thread_destroy( BOOLEAN last )
 {
+    HEAP_lfh_notify_thread_destroy( last );
 }
diff --git a/dlls/ntdll/heap_lfh.c b/dlls/ntdll/heap_lfh.c
new file mode 100644
index 00000000000..11111111111
--- /dev/null
+++ b/dlls/ntdll/heap_lfh.c
@@ -0,0 +1,1228 @@
+/*
+ * Wine Low Fragmentation Heap
+ *
+ * Copyright 2020 Remi Bernon for CodeWeavers
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
+ */
+
+#include "ntstatus.h"
+#define WIN32_NO_STATUS
+
+#include "wine/list.h"
+#include "wine/debug.h"
+
+#include "ntdll_misc.h"
+
+WINE_DEFAULT_DEBUG_CHANNEL(heap);
+
+#define ROUND_SIZE(size, mask) ((((SIZE_T)(size) + (mask)) & ~(SIZE_T)(mask)))
+#define ALIGNMENT (2 * sizeof(void *))
+
+#define BLOCK_FLAG_USER_INFO   0x00000010 /* user flags up to 0xf0 */
+#define BLOCK_FLAG_USER_MASK   0x000000f0
+
+#define BLOCK_USER_FLAGS( heap_flags ) (((heap_flags) >> 4) & BLOCK_FLAG_USER_MASK)
+#define HEAP_USER_FLAGS( block_flags ) (((block_flags) & BLOCK_FLAG_USER_MASK) << 4)
+
+typedef struct LFH_ptr LFH_ptr;
+typedef struct LFH_block LFH_block;
+typedef enum LFH_block_type LFH_block_type;
+typedef struct LFH_arena LFH_arena;
+typedef struct LFH_class LFH_class;
+typedef struct LFH_heap LFH_heap;
+typedef struct LFH_slist LFH_slist;
+
+#define ARENA_HEADER_SIZE (sizeof(LFH_arena))
+
+#define LARGE_ARENA_SIZE 0x400000 /* 4MiB */
+#define LARGE_ARENA_MASK (LARGE_ARENA_SIZE - 1)
+
+#define BLOCK_ARENA_SIZE 0x10000 /* 64kiB */
+#define BLOCK_ARENA_MASK (BLOCK_ARENA_SIZE - 1)
+
+#define SMALL_CLASS_STEP     0x20
+#define SMALL_CLASS_MASK     (SMALL_CLASS_STEP - 1)
+#define SMALL_CLASS_MIN_SIZE SMALL_CLASS_STEP
+#define SMALL_CLASS_MAX_SIZE 0x800
+#define SMALL_CLASS_COUNT    ((SMALL_CLASS_MAX_SIZE - SMALL_CLASS_MIN_SIZE) / SMALL_CLASS_STEP + 1)
+#define SMALL_CLASS_FIRST    0
+#define SMALL_CLASS_LAST     (SMALL_CLASS_FIRST + SMALL_CLASS_COUNT - 1)
+
+#define MEDIUM_CLASS_STEP     (16 * SMALL_CLASS_STEP)
+#define MEDIUM_CLASS_MASK     (MEDIUM_CLASS_STEP - 1)
+#define MEDIUM_CLASS_MIN_SIZE SMALL_CLASS_MAX_SIZE
+#define MEDIUM_CLASS_MAX_SIZE ((BLOCK_ARENA_SIZE - ARENA_HEADER_SIZE - ARENA_HEADER_SIZE) / 2)
+#define MEDIUM_CLASS_COUNT    ((MEDIUM_CLASS_MAX_SIZE - MEDIUM_CLASS_MIN_SIZE + MEDIUM_CLASS_MASK) / MEDIUM_CLASS_STEP + 1)
+#define MEDIUM_CLASS_FIRST    (SMALL_CLASS_LAST + 1)
+#define MEDIUM_CLASS_LAST     (MEDIUM_CLASS_FIRST + MEDIUM_CLASS_COUNT - 1)
+
+#define LARGE_CLASS_STEP      BLOCK_ARENA_SIZE
+#define LARGE_CLASS_MASK      (LARGE_CLASS_STEP - 1)
+#define LARGE_CLASS_MIN_SIZE  (BLOCK_ARENA_SIZE - ARENA_HEADER_SIZE)
+#define LARGE_CLASS_MAX_SIZE  (LARGE_ARENA_SIZE / 2 - ARENA_HEADER_SIZE) /* we need an arena header for every large block */
+#define LARGE_CLASS_COUNT     ((LARGE_CLASS_MAX_SIZE - LARGE_CLASS_MIN_SIZE) / LARGE_CLASS_STEP + 1)
+#define LARGE_CLASS_FIRST     0
+#define LARGE_CLASS_LAST      (LARGE_CLASS_FIRST + LARGE_CLASS_COUNT - 1)
+
+#define TOTAL_BLOCK_CLASS_COUNT (MEDIUM_CLASS_LAST + 1)
+#define TOTAL_LARGE_CLASS_COUNT (LARGE_CLASS_LAST + 1)
+
+struct LFH_slist
+{
+    LFH_slist *next;
+};
+
+static inline void LFH_slist_push(LFH_slist **list, LFH_slist *entry)
+{
+    /* There will be no ABA issue here, other threads can only replace
+     * list->next with a different entry, or NULL. */
+    entry->next = __atomic_load_n(list, __ATOMIC_RELAXED);
+    while (!__atomic_compare_exchange_n(list, &entry->next, entry, 0, __ATOMIC_RELEASE, __ATOMIC_ACQUIRE));
+}
+
+static inline LFH_slist *LFH_slist_flush(LFH_slist **list)
+{
+    if (!__atomic_load_n(list, __ATOMIC_RELAXED)) return NULL;
+    return __atomic_exchange_n(list, NULL, __ATOMIC_ACQUIRE);
+}
+
+/* be sure to keep these different from ARENA_INUSE magic */
+enum LFH_block_type
+{
+    LFH_block_type_used = 0xa55a5aa5,
+    LFH_block_type_free = 0xc33c3cc3,
+};
+
+struct DECLSPEC_ALIGN(16) LFH_block
+{
+    union
+    {
+        ssize_t next_free;
+        LFH_slist entry_defer;
+        size_t alloc_size;
+    };
+
+    UINT32 flags;
+    LFH_block_type type;
+};
+
+C_ASSERT(sizeof(LFH_block) == 0x10);
+C_ASSERT(offsetof(LFH_block, entry_defer) == 0);
+
+struct DECLSPEC_ALIGN(16) LFH_arena
+{
+    ssize_t next_free;
+    LFH_arena *class_entry;
+
+    union
+    {
+        LFH_arena *parent;
+        LFH_class *class;
+    };
+
+    union
+    {
+        size_t huge_size;
+        size_t used_count;
+    };
+};
+
+#ifdef _WIN64
+C_ASSERT(sizeof(LFH_arena) == 0x20);
+#else
+C_ASSERT(sizeof(LFH_arena) == 0x10);
+#endif
+
+struct LFH_class
+{
+    LFH_arena *next;
+    size_t     size;
+};
+
+struct LFH_heap
+{
+    LFH_slist *list_defer;
+    LFH_arena *cached_large_arena;
+
+    LFH_class block_class[TOTAL_BLOCK_CLASS_COUNT];
+    LFH_class large_class[TOTAL_LARGE_CLASS_COUNT];
+
+    SLIST_ENTRY entry_orphan;
+#ifdef _WIN64
+    void *pad[0xc2];
+#else
+    void *pad[0xc3];
+#endif
+};
+
+C_ASSERT(TOTAL_BLOCK_CLASS_COUNT == 0x7d);
+C_ASSERT(TOTAL_LARGE_CLASS_COUNT == 0x20);
+
+/* arena->class/arena->parent pointer low bits are used to discriminate between the two */
+C_ASSERT(offsetof(LFH_heap, block_class[0]) > 0);
+C_ASSERT(offsetof(LFH_heap, large_class[TOTAL_LARGE_CLASS_COUNT]) < BLOCK_ARENA_SIZE);
+
+/* helpers to retrieve parent arena from a child, or class pointer from a large or block arena */
+static inline LFH_arena *LFH_parent_from_arena(const LFH_arena *arena)
+{
+    if (!arena->parent) return (LFH_arena *)arena;
+    if (!((UINT_PTR)(arena)->parent & BLOCK_ARENA_MASK)) return arena->parent;
+    return (LFH_arena *)arena;
+}
+
+static inline LFH_class *LFH_class_from_arena(const LFH_arena *arena)
+{
+    const LFH_arena *parent = LFH_parent_from_arena(arena);
+    if ((UINT_PTR)parent->class & BLOCK_ARENA_MASK) return parent->class;
+    return NULL;
+}
+
+/* make sure its aligns to power of two so we can mask class pointers in LFH_heap_from_arena */
+#ifdef _WIN64
+C_ASSERT(sizeof(LFH_heap) == 0x1000);
+#else
+C_ASSERT(sizeof(LFH_heap) == 0x800);
+#endif
+
+/* helper to retrieve the heap from an arena, using its class pointer */
+static inline LFH_heap *LFH_heap_from_arena(const LFH_arena *arena)
+{
+    LFH_class *class = LFH_class_from_arena(arena);
+    return (LFH_heap *)((UINT_PTR)class & ~(sizeof(LFH_heap) - 1));
+}
+
+/* helpers to retrieve block pointers to the containing block or large (maybe child) arena */
+static inline LFH_arena *LFH_large_arena_from_block(const LFH_block *block)
+{
+    return (LFH_arena *)((UINT_PTR)block & ~BLOCK_ARENA_MASK);
+}
+
+static inline LFH_arena *LFH_block_arena_from_block(const LFH_block *block)
+{
+    return LFH_large_arena_from_block(block) + 1;
+}
+
+static inline LFH_arena *LFH_arena_from_block(const LFH_block *block)
+{
+    LFH_arena *block_arena = LFH_block_arena_from_block(block);
+    if (block_arena == (LFH_arena *)block) return LFH_large_arena_from_block(block);
+    return block_arena;
+}
+
+/* helpers to translate between data pointer and LFH_block header */
+static inline LFH_block *LFH_block_from_ptr(const LFH_ptr *ptr)
+{
+    return ((LFH_block *)ptr) - 1;
+}
+
+static inline void *LFH_ptr_from_block(const LFH_block *block)
+{
+    return (LFH_ptr *)(block + 1);
+}
+
+static inline size_t LFH_block_get_class_size(const LFH_block *block)
+{
+    const LFH_arena *arena = LFH_arena_from_block(block);
+    const LFH_class *class = LFH_class_from_arena(arena);
+    if (class) return class->size;
+    return arena->huge_size;
+}
+
+static inline size_t LFH_block_get_alloc_size(const LFH_block *block, ULONG flags)
+{
+    return block->alloc_size;
+}
+
+static inline size_t LFH_get_class_size(ULONG flags, size_t size)
+{
+    static const ULONG padd_flags = HEAP_VALIDATE | HEAP_VALIDATE_ALL | HEAP_VALIDATE_PARAMS | HEAP_ADD_USER_INFO;
+    static const ULONG check_flags = HEAP_TAIL_CHECKING_ENABLED | HEAP_FREE_CHECKING_ENABLED | HEAP_CHECKING_ENABLED;
+    SIZE_T overhead;
+
+    if ((flags & check_flags)) overhead = ALIGNMENT;
+    else overhead = sizeof(LFH_block);
+
+    if (flags & HEAP_TAIL_CHECKING_ENABLED) overhead += ALIGNMENT;
+    if (flags & padd_flags) overhead += ALIGNMENT;
+
+    if (size < ALIGNMENT) size = ALIGNMENT;
+    return ROUND_SIZE( size + overhead, ALIGNMENT - 1 );
+}
+
+static inline void *LFH_memory_allocate(size_t size)
+{
+    void *addr = NULL;
+    SIZE_T alloc_size = size;
+
+    if (NtAllocateVirtualMemory(NtCurrentProcess(), (void **)&addr, 0, &alloc_size,
+                                MEM_RESERVE | MEM_COMMIT, PAGE_READWRITE))
+        return NULL;
+
+    return addr;
+}
+
+static inline BOOLEAN LFH_memory_deallocate(void *addr, size_t size)
+{
+    SIZE_T release_size = 0;
+
+    if (NtFreeVirtualMemory(NtCurrentProcess(), &addr, &release_size, MEM_RELEASE))
+        return FALSE;
+
+    return TRUE;
+}
+
+static inline LFH_block *LFH_arena_get_block(const LFH_arena *arena, size_t offset)
+{
+    return (LFH_block *)((UINT_PTR)arena + offset);
+}
+
+static inline void LFH_arena_push_block(LFH_arena *arena, LFH_block *block)
+{
+    block->flags = 0;
+    block->type = LFH_block_type_free;
+    block->next_free = arena->next_free;
+    arena->next_free = (UINT_PTR)block - (UINT_PTR)arena;
+    arena->used_count--;
+}
+
+static inline LFH_block *LFH_arena_pop_block(LFH_arena *arena)
+{
+    if (arena->next_free > 0)
+    {
+        LFH_block *block = LFH_arena_get_block(arena, arena->next_free);
+        arena->next_free = block->next_free;
+        arena->used_count++;
+        return block;
+    }
+    else
+    {
+        LFH_arena *child, *large_arena = LFH_large_arena_from_block((LFH_block *)arena);
+        LFH_class *class = LFH_class_from_arena(arena);
+        LFH_block *block = LFH_arena_get_block(arena, -arena->next_free);
+        ssize_t extra = 0, limit;
+
+        if (arena == large_arena)
+        {
+            extra = ARENA_HEADER_SIZE;
+            limit = LARGE_ARENA_SIZE;
+            child = LFH_large_arena_from_block(block);
+            if (arena != child) child->parent = arena;
+        }
+        else limit = LFH_class_from_arena(large_arena)->size;
+
+        arena->next_free -= class->size + extra;
+        if (-arena->next_free > limit - class->size)
+            arena->next_free = 0;
+
+        arena->used_count++;
+        return block;
+    }
+}
+
+static inline int LFH_arena_is_empty(LFH_arena *arena)
+{
+    return arena->next_free == 0;
+}
+
+static inline int LFH_arena_is_used(LFH_arena *arena)
+{
+    return arena->used_count > 0;
+}
+
+static inline int LFH_class_is_block(LFH_heap *heap, LFH_class *class)
+{
+    return class >= heap->block_class && class < (heap->block_class + TOTAL_BLOCK_CLASS_COUNT);
+}
+
+static void LFH_class_initialize(LFH_heap *heap, LFH_class *class, size_t index)
+{
+    class->next = NULL;
+
+    if (LFH_class_is_block(heap, class))
+    {
+        if (index <= SMALL_CLASS_LAST)
+            class->size = min(SMALL_CLASS_MIN_SIZE + SMALL_CLASS_STEP * (index - SMALL_CLASS_FIRST), SMALL_CLASS_MAX_SIZE);
+        else
+            class->size = min(MEDIUM_CLASS_MIN_SIZE + MEDIUM_CLASS_STEP * (index - MEDIUM_CLASS_FIRST), MEDIUM_CLASS_MAX_SIZE);
+    }
+    else
+    {
+        class->size = min(LARGE_CLASS_MIN_SIZE + LARGE_CLASS_STEP * (index - LARGE_CLASS_FIRST), LARGE_CLASS_MAX_SIZE);
+    }
+}
+
+static inline LFH_arena *LFH_class_pop_arena(LFH_class *class)
+{
+    LFH_arena *arena = class->next;
+    if (!arena) return NULL;
+    class->next = arena->class_entry;
+    return arena;
+}
+
+static inline void LFH_class_remove_arena(LFH_class *class, LFH_arena *arena)
+{
+    LFH_arena **next = &class->next;
+    while (*next != arena) next = &(*next)->class_entry;
+    *next = arena->class_entry;
+}
+
+static inline LFH_arena *LFH_class_peek_arena(LFH_class *class)
+{
+    return class->next;
+}
+
+static inline void LFH_class_push_arena(LFH_class *class, LFH_arena *arena)
+{
+    arena->class_entry = class->next;
+    class->next = arena;
+}
+
+static inline LFH_class *LFH_heap_get_class(LFH_heap *heap, size_t size)
+{
+    if (size == 0)
+        return &heap->block_class[0];
+    else if (size <= SMALL_CLASS_MAX_SIZE)
+        return &heap->block_class[SMALL_CLASS_FIRST + (size + SMALL_CLASS_MASK - SMALL_CLASS_MIN_SIZE) / SMALL_CLASS_STEP];
+    else if (size <= MEDIUM_CLASS_MAX_SIZE)
+        return &heap->block_class[MEDIUM_CLASS_FIRST + (size + MEDIUM_CLASS_MASK - MEDIUM_CLASS_MIN_SIZE) / MEDIUM_CLASS_STEP];
+    else if (size <= LARGE_CLASS_MAX_SIZE)
+        return &heap->large_class[LARGE_CLASS_FIRST + (size + LARGE_CLASS_MASK - LARGE_CLASS_MIN_SIZE) / LARGE_CLASS_STEP];
+    else
+        return NULL;
+}
+
+static void LFH_arena_initialize(LFH_heap *heap, LFH_class *class, LFH_arena *arena, size_t huge_size)
+{
+    arena->class = class;
+    arena->next_free = -ARENA_HEADER_SIZE;
+
+    if (class == NULL)
+        arena->huge_size = huge_size;
+    else
+        arena->used_count = 0;
+}
+
+static LFH_arena *LFH_acquire_arena(LFH_heap *heap, LFH_class *class);
+static BOOLEAN LFH_release_arena(LFH_heap *heap, LFH_arena *arena);
+
+static inline LFH_block *LFH_allocate_block(LFH_heap *heap, LFH_class *class, LFH_arena *arena);
+static inline BOOLEAN LFH_deallocate_block(LFH_heap *heap, LFH_arena *arena, LFH_block *block);
+
+static inline BOOLEAN LFH_deallocate_deferred_blocks(LFH_heap *heap)
+{
+    LFH_slist *entry = LFH_slist_flush(&heap->list_defer);
+
+    while (entry)
+    {
+        LFH_block *block = LIST_ENTRY(entry, LFH_block, entry_defer);
+        entry = entry->next;
+
+        if (!LFH_deallocate_block(heap, LFH_arena_from_block(block), block))
+            return FALSE;
+    }
+
+    return TRUE;
+}
+
+static inline void LFH_deallocated_cached_arenas(LFH_heap *heap)
+{
+    if (!heap->cached_large_arena) return;
+    LFH_memory_deallocate(heap->cached_large_arena, LARGE_ARENA_SIZE);
+    heap->cached_large_arena = NULL;
+}
+
+static inline size_t LFH_huge_alloc_size(size_t size)
+{
+    return (ARENA_HEADER_SIZE + size + BLOCK_ARENA_MASK) & ~BLOCK_ARENA_MASK;
+}
+
+static inline LFH_arena *LFH_allocate_huge_arena(LFH_heap *heap, size_t size)
+{
+    LFH_arena *arena;
+    size_t alloc_size = LFH_huge_alloc_size(size);
+    if (alloc_size < size) return NULL;
+
+    if ((arena = LFH_memory_allocate(alloc_size)))
+        LFH_arena_initialize(heap, NULL, arena, size);
+
+    return arena;
+}
+
+static inline LFH_arena *LFH_allocate_large_arena(LFH_heap *heap, LFH_class *class)
+{
+    LFH_arena *arena;
+
+    if ((arena = heap->cached_large_arena) ||
+        (arena = LFH_memory_allocate(LARGE_ARENA_SIZE)))
+    {
+        heap->cached_large_arena = NULL;
+        LFH_arena_initialize(heap, class, arena, 0);
+        LFH_class_push_arena(class, arena);
+    }
+
+    return arena;
+}
+
+static inline LFH_arena *LFH_allocate_block_arena(LFH_heap *heap, LFH_class *large_class, LFH_class *block_class)
+{
+    LFH_arena *large_arena;
+    LFH_arena *arena = NULL;
+
+    if ((large_arena = LFH_acquire_arena(heap, large_class)))
+    {
+        arena = (LFH_arena *)LFH_allocate_block(heap, large_class, large_arena);
+        LFH_arena_initialize(heap, block_class, arena, 0);
+        LFH_class_push_arena(block_class, arena);
+    }
+
+    return arena;
+}
+
+static inline LFH_arena *LFH_acquire_arena(LFH_heap *heap, LFH_class *class)
+{
+    LFH_arena *arena;
+
+    if (!(arena = LFH_class_peek_arena(class)))
+    {
+        if (LFH_class_is_block(heap, class))
+            arena = LFH_allocate_block_arena(heap, &heap->large_class[0], class);
+        else
+            arena = LFH_allocate_large_arena(heap, class);
+    }
+
+    return arena;
+}
+
+static inline BOOLEAN LFH_release_arena(LFH_heap *heap, LFH_arena *arena)
+{
+    LFH_arena *large_arena = LFH_large_arena_from_block((LFH_block *)arena);
+    if (arena == large_arena && !heap->cached_large_arena)
+    {
+        heap->cached_large_arena = arena;
+        return TRUE;
+    }
+    else if (arena == large_arena)
+        return LFH_memory_deallocate(arena, LARGE_ARENA_SIZE);
+    else
+        return LFH_deallocate_block(heap, large_arena, (LFH_block *)arena);
+};
+
+static inline LFH_block *LFH_allocate_block(LFH_heap *heap, LFH_class *class, LFH_arena *arena)
+{
+    LFH_block *block = LFH_arena_pop_block(arena);
+    if (LFH_arena_is_empty(arena))
+        LFH_class_pop_arena(class);
+    return block;
+}
+
+static inline BOOLEAN LFH_deallocate_block(LFH_heap *heap, LFH_arena *arena, LFH_block *block)
+{
+    LFH_class *class = LFH_class_from_arena(arena);
+
+    arena = LFH_parent_from_arena(arena);
+    if (LFH_arena_is_empty(arena))
+        LFH_class_push_arena(class, arena);
+
+    LFH_arena_push_block(arena, block);
+    if (LFH_arena_is_used(arena))
+        return TRUE;
+
+    LFH_class_remove_arena(class, arena);
+    return LFH_release_arena(heap, arena);
+}
+
+static void LFH_heap_initialize(LFH_heap *heap)
+{
+    size_t i;
+
+    for (i = 0; i < TOTAL_LARGE_CLASS_COUNT; ++i)
+        LFH_class_initialize(heap, &heap->large_class[i], i);
+    for (i = 0; i < TOTAL_BLOCK_CLASS_COUNT; ++i)
+        LFH_class_initialize(heap, &heap->block_class[i], i);
+
+    heap->list_defer = NULL;
+    heap->cached_large_arena = NULL;
+}
+
+static SLIST_HEADER *LFH_orphan_list(void)
+{
+    static SLIST_HEADER *header;
+    SLIST_HEADER *ptr, *expected = NULL;
+    LFH_heap *tmp;
+
+    C_ASSERT(sizeof(LFH_heap) >= sizeof(SLIST_HEADER));
+
+    if ((ptr = __atomic_load_n(&header, __ATOMIC_RELAXED)))
+        return ptr;
+
+    if (!(ptr = LFH_memory_allocate(BLOCK_ARENA_SIZE)))
+        return NULL;
+
+    RtlInitializeSListHead(ptr);
+    for (tmp = (LFH_heap *)ptr + 1; tmp < (LFH_heap *)ptr + BLOCK_ARENA_SIZE / sizeof(*tmp); tmp++)
+    {
+        LFH_heap_initialize(tmp);
+        RtlInterlockedPushEntrySList(ptr, &tmp->entry_orphan);
+    }
+
+    if (__atomic_compare_exchange_n(&header, &expected, ptr, 0, __ATOMIC_RELEASE, __ATOMIC_ACQUIRE))
+        return ptr;
+
+    LFH_memory_deallocate(ptr, BLOCK_ARENA_SIZE);
+    return expected;
+}
+
+static void LFH_heap_finalize(LFH_heap *heap)
+{
+    LFH_arena *arena;
+
+    LFH_deallocate_deferred_blocks(heap);
+
+    for (size_t i = 0; i < TOTAL_BLOCK_CLASS_COUNT; ++i)
+    {
+        while ((arena = LFH_class_pop_arena(&heap->block_class[i])))
+        {
+            WARN("block arena %p still has used blocks\n", arena);
+            LFH_release_arena(heap, arena);
+        }
+    }
+
+    for (size_t i = 0; i < TOTAL_LARGE_CLASS_COUNT; ++i)
+    {
+        while ((arena = LFH_class_pop_arena(&heap->large_class[i])))
+        {
+            WARN("large arena %p still has used blocks\n", arena);
+            LFH_memory_deallocate(arena, LARGE_ARENA_SIZE);
+        }
+    }
+
+    LFH_deallocated_cached_arenas(heap);
+}
+
+static LFH_heap *LFH_heap_allocate(void)
+{
+    SLIST_HEADER *list_orphan = LFH_orphan_list();
+    LFH_heap *heap, *tmp;
+
+    heap = LFH_memory_allocate(BLOCK_ARENA_SIZE);
+    if (!heap)
+        return NULL;
+
+    for (tmp = heap + 1; tmp < heap + BLOCK_ARENA_SIZE / sizeof(*tmp); tmp++)
+    {
+        LFH_heap_initialize(tmp);
+        RtlInterlockedPushEntrySList(list_orphan, &tmp->entry_orphan);
+    }
+
+    LFH_heap_initialize(heap);
+    return heap;
+}
+
+static LFH_heap *LFH_create_thread_heap(void)
+{
+    SLIST_ENTRY *entry;
+    LFH_heap *heap;
+
+    if ((entry = RtlInterlockedPopEntrySList(LFH_orphan_list())))
+        heap = LIST_ENTRY(entry, LFH_heap, entry_orphan);
+    else
+        heap = LFH_heap_allocate();
+
+    return (NtCurrentTeb()->Reserved5[2] = heap);
+}
+
+static inline LFH_heap *LFH_thread_heap(BOOL create)
+{
+    LFH_heap *heap = (LFH_heap *)NtCurrentTeb()->Reserved5[2];
+    if (!heap && create) return LFH_create_thread_heap();
+    return heap;
+}
+
+static void LFH_dump_arena(LFH_heap *heap, LFH_class *class, LFH_arena *arena)
+{
+    LFH_arena *large_arena = LFH_large_arena_from_block((LFH_block *)arena);
+    LFH_arena *block_arena = LFH_block_arena_from_block((LFH_block *)arena);
+
+    if (arena == block_arena)
+        WARN("    block arena: %p-%p", arena, (void *)((UINT_PTR)large_arena + BLOCK_ARENA_SIZE - 1));
+    else if (arena == large_arena)
+        WARN("    large arena: %p-%p", arena, (void *)((UINT_PTR)large_arena + LARGE_ARENA_SIZE - 1));
+
+    WARN(" heap: %p class: %p parent: %p free: %Id used: %Id\n",
+          LFH_heap_from_arena(arena), LFH_class_from_arena(arena), LFH_parent_from_arena(arena), arena->next_free, arena->used_count);
+}
+
+static void LFH_dump_class(LFH_heap *heap, LFH_class *class)
+{
+    LFH_arena *arena = class->next;
+    if (!arena) return;
+
+    if (LFH_class_is_block(heap, class))
+        WARN("  block class: %p size: %Ix\n", class, class->size);
+    else
+        WARN("  large class: %p size: %Ix\n", class, class->size);
+
+    while (arena)
+    {
+        LFH_dump_arena(heap, class, arena);
+        arena = arena->class_entry;
+    }
+}
+
+static void LFH_dump_heap(LFH_heap *heap)
+{
+    size_t i;
+
+    WARN("heap: %p\n", heap);
+
+    for (i = 0; i < TOTAL_BLOCK_CLASS_COUNT; ++i)
+        LFH_dump_class(heap, &heap->block_class[i]);
+
+    for (i = 0; i < TOTAL_LARGE_CLASS_COUNT; ++i)
+        LFH_dump_class(heap, &heap->large_class[i]);
+}
+
+static BOOLEAN LFH_validate_arena(ULONG flags, const LFH_arena *arena);
+static BOOLEAN LFH_validate_heap(ULONG flags, const LFH_heap *heap);
+
+static inline BOOLEAN LFH_validate_block(ULONG flags, const LFH_block *block)
+{
+    const LFH_arena *arena = LFH_arena_from_block(block);
+    const LFH_arena *large_arena = LFH_large_arena_from_block(block);
+    const LFH_arena *block_arena = LFH_block_arena_from_block(block);
+    const LFH_arena *arena_arena = LFH_large_arena_from_block((LFH_block *)arena);
+    const char *err = NULL;
+
+    if (flags & HEAP_VALIDATE)
+        return LFH_validate_arena(flags, arena);
+
+    if (!arena)
+        err = "invalid arena";
+    else if (arena != arena_arena && arena != (arena_arena + 1))
+        err = "invalid arena alignment";
+    else if (arena == block_arena)
+    {
+        if ((UINT_PTR)block < (UINT_PTR)block_arena + ARENA_HEADER_SIZE)
+            err = "invalid block alignment";
+        if (((UINT_PTR)block & (sizeof(*block) - 1)))
+            err = "invalid block alignment";
+    }
+    else if (arena != large_arena)
+        err = "large/huge arena mismatch";
+    else if ((UINT_PTR)block != (UINT_PTR)block_arena)
+        err = "invalid block for large/huge arena";
+
+    if (err) WARN("%08lx %p: %s\n", flags, block, err);
+    return err == NULL;
+}
+
+static BOOLEAN LFH_validate_free_block(ULONG flags, const LFH_block *block)
+{
+    const char *err = NULL;
+
+    if (!LFH_validate_block(flags, block))
+        return FALSE;
+    if (block->type != LFH_block_type_free)
+        err = "invalid free block type";
+
+    if (err) WARN("%08lx %p: %s\n", flags, block, err);
+    return err == NULL;
+}
+
+static BOOLEAN LFH_validate_defer_block(ULONG flags, const LFH_block *block)
+{
+    const char *err = NULL;
+
+    if (!LFH_validate_block(flags, block))
+        return FALSE;
+    if (block->type != LFH_block_type_free)
+        err = "invalid defer block type";
+    else if (flags & HEAP_FREE_CHECKING_ENABLED)
+    {
+        const unsigned int *data = (const unsigned int *)LFH_ptr_from_block(block);
+        size_t class_size = LFH_block_get_class_size(block);
+        for (size_t i = 0; i < class_size / 4 - (data - (const unsigned int *)block) && !err; ++i)
+            if (data[i] != 0xfeeefeee) err = "invalid free filler";
+    }
+
+    if (err) WARN("%08lx %p: %s\n", flags, block, err);
+    return err == NULL;
+}
+
+static inline BOOLEAN LFH_validate_used_block(ULONG flags, const LFH_block *block)
+{
+    const char *err = NULL;
+
+    if (!LFH_validate_block(flags, block))
+        return FALSE;
+    if (block->type != LFH_block_type_used)
+        err = "invalid used block type";
+    else if (flags & HEAP_TAIL_CHECKING_ENABLED)
+    {
+        const unsigned char *data = (const unsigned char *)LFH_ptr_from_block(block);
+        size_t alloc_size = LFH_block_get_alloc_size(block, flags);
+        for (size_t i = alloc_size; i < alloc_size + ALIGNMENT && !err; ++i)
+            if (data[i] != 0xab) err = "invalid tail filler";
+    }
+
+    if (err) WARN("%08lx %p: %s\n", flags, block, err);
+    return err == NULL;
+}
+
+static BOOLEAN LFH_validate_arena_free_blocks(ULONG flags, const LFH_arena *arena)
+{
+    ssize_t offset = arena->next_free;
+    while (offset > 0)
+    {
+        LFH_block *block = LFH_arena_get_block(arena, offset);
+        if (!LFH_validate_free_block(flags, block))
+            return FALSE;
+        offset = block->next_free;
+    }
+
+    return TRUE;
+}
+
+static BOOLEAN LFH_validate_arena(ULONG flags, const LFH_arena *arena)
+{
+    const char *err = NULL;
+    const LFH_arena *parent;
+    const LFH_arena *block_arena = LFH_block_arena_from_block((LFH_block *)arena);
+    const LFH_arena *large_arena = LFH_large_arena_from_block((LFH_block *)arena);
+
+    if (flags & HEAP_VALIDATE)
+        return LFH_validate_heap(flags, LFH_heap_from_arena(arena));
+
+    if (arena != large_arena && arena != block_arena)
+        err = "invalid arena alignment";
+    else if (arena == block_arena)
+    {
+        if (!LFH_validate_block(flags, (LFH_block *)arena))
+            err = "invalid block arena";
+        else if (!LFH_validate_arena_free_blocks(flags, arena))
+            err = "invalid block arena free list";
+    }
+    else if (arena == large_arena && !LFH_class_from_arena(arena))
+    {
+        if (arena->huge_size <= LARGE_CLASS_MAX_SIZE)
+            err = "invalid huge arena size";
+    }
+    else if (arena == large_arena && (parent = LFH_parent_from_arena(arena)) != arena)
+    {
+        if (arena > parent || LFH_large_arena_from_block((LFH_block *)parent) != parent)
+            err = "invalid child arena parent";
+    }
+    else
+    {
+        if (!LFH_validate_arena_free_blocks(flags, arena))
+            err = "invalid large arena free list";
+    }
+
+    if (err) WARN("%08lx %p: %s\n", flags, arena, err);
+    return err == NULL;
+}
+
+static BOOLEAN LFH_validate_class_arenas(ULONG flags, const LFH_class *class)
+{
+    LFH_arena *arena = class->next;
+    while (arena)
+    {
+        if (!LFH_validate_arena(flags, arena))
+            return FALSE;
+
+        arena = arena->class_entry;
+    }
+
+    return TRUE;
+}
+
+static BOOLEAN LFH_validate_heap_defer_blocks(ULONG flags, const LFH_heap *heap)
+{
+    const LFH_slist *entry = heap->list_defer;
+
+    while (entry)
+    {
+        const LFH_block *block = LIST_ENTRY(entry, LFH_block, entry_defer);
+        if (!LFH_validate_defer_block(flags, block))
+            return FALSE;
+        entry = entry->next;
+    }
+
+    return TRUE;
+}
+
+static BOOLEAN LFH_validate_heap(ULONG flags, const LFH_heap *heap)
+{
+    const char *err = NULL;
+    UINT i;
+
+    flags &= ~HEAP_VALIDATE;
+
+    if (heap != LFH_thread_heap(FALSE))
+        err = "unable to validate foreign heap";
+    else if (!LFH_validate_heap_defer_blocks(flags, heap))
+        err = "invalid heap defer blocks";
+    else
+    {
+        for (i = 0; err == NULL && i < TOTAL_BLOCK_CLASS_COUNT; ++i)
+        {
+            if (!LFH_validate_class_arenas(flags, &heap->block_class[i]))
+                return FALSE;
+        }
+
+        for (i = 0; err == NULL && i < TOTAL_LARGE_CLASS_COUNT; ++i)
+        {
+            if (!LFH_validate_class_arenas(flags, &heap->large_class[i]))
+                return FALSE;
+        }
+    }
+
+    if (err) WARN("%08lx %p: %s\n", flags, heap, err);
+    return err == NULL;
+}
+
+static inline void LFH_block_initialize(LFH_block *block, ULONG flags, size_t old_size, size_t new_size, size_t class_size)
+{
+    char *ptr = (char *)LFH_ptr_from_block(block);
+
+    TRACE("block %p, flags %08lx, old_size %Ix, new_size %Ix, class_size %Ix, ptr %p\n", block, flags, old_size, new_size, class_size, ptr);
+
+    if ((flags & HEAP_ZERO_MEMORY) && new_size > old_size)
+        memset(ptr + old_size, 0, new_size - old_size);
+    else if ((flags & HEAP_FREE_CHECKING_ENABLED) && new_size > old_size && class_size < BLOCK_ARENA_SIZE)
+        memset(ptr + old_size, 0x55, new_size - old_size);
+
+    if ((flags & HEAP_TAIL_CHECKING_ENABLED))
+        memset(ptr + new_size, 0xab, class_size - new_size - (ptr - (char *)block));
+
+    block->flags = BLOCK_USER_FLAGS( flags );
+    block->type = LFH_block_type_used;
+    block->alloc_size = new_size;
+}
+
+static FORCEINLINE LFH_ptr *LFH_allocate(ULONG flags, size_t size)
+{
+    LFH_block *block = NULL;
+    LFH_class *class;
+    LFH_arena *arena;
+    LFH_heap *heap = LFH_thread_heap(TRUE);
+    size_t class_size = LFH_get_class_size(flags, size);
+
+    if (class_size == ~(size_t)0)
+        return NULL;
+
+    if (!LFH_deallocate_deferred_blocks(heap))
+        return NULL;
+
+    if ((class = LFH_heap_get_class(heap, class_size)))
+    {
+        arena = LFH_acquire_arena(heap, class);
+        if (arena) block = LFH_allocate_block(heap, class, arena);
+        if (block) LFH_block_initialize(block, flags, 0, size, LFH_block_get_class_size(block));
+    }
+    else
+    {
+        arena = LFH_allocate_huge_arena(heap, class_size);
+        if (arena) block = LFH_arena_get_block(arena, ARENA_HEADER_SIZE);
+        if (block) LFH_block_initialize(block, flags & ~HEAP_ZERO_MEMORY, 0, size, LFH_block_get_class_size(block));
+    }
+
+    LFH_deallocated_cached_arenas(heap);
+
+    if (!block) return NULL;
+    return LFH_ptr_from_block(block);
+}
+
+static FORCEINLINE BOOLEAN LFH_free(ULONG flags, LFH_ptr *ptr)
+{
+    LFH_block *block = LFH_block_from_ptr(ptr);
+    LFH_arena *arena = LFH_arena_from_block(block);
+    LFH_heap *heap = LFH_heap_from_arena(arena);
+
+    if (!LFH_class_from_arena(arena))
+        return LFH_memory_deallocate(arena, LFH_block_get_class_size(block));
+
+    if (flags & HEAP_FREE_CHECKING_ENABLED)
+    {
+        unsigned int *data = (unsigned int *)LFH_ptr_from_block(block);
+        size_t class_size = LFH_block_get_class_size(block);
+        for (size_t i = 0; i < class_size / 4 - (data - (const unsigned int *)block); ++i)
+            data[i] = 0xfeeefeee;
+    }
+
+    block->flags = 0;
+    block->type = LFH_block_type_free;
+
+    if (heap == LFH_thread_heap(FALSE) && !(flags & HEAP_FREE_CHECKING_ENABLED))
+        LFH_deallocate_block(heap, LFH_arena_from_block(block), block);
+    else
+        LFH_slist_push(&heap->list_defer, &block->entry_defer);
+
+    return TRUE;
+}
+
+static FORCEINLINE LFH_ptr *LFH_reallocate(ULONG flags, LFH_ptr *old_ptr, size_t new_size)
+{
+    LFH_block *block = LFH_block_from_ptr(old_ptr);
+    LFH_arena *arena = LFH_arena_from_block(block);
+    LFH_heap *heap = LFH_heap_from_arena(arena);
+    size_t old_size = LFH_block_get_alloc_size(block, flags);
+    size_t old_class_size = LFH_block_get_class_size(block);
+    size_t new_class_size = LFH_get_class_size(flags, new_size);
+    LFH_class *new_class, *old_class = LFH_class_from_arena(arena);
+    LFH_ptr *new_ptr = NULL;
+
+    if (new_class_size == ~(size_t)0)
+        return NULL;
+
+    if (new_class_size <= old_class_size)
+        goto in_place;
+
+    if ((new_class = LFH_heap_get_class(heap, new_class_size)) && new_class == old_class)
+        goto in_place;
+
+    old_class_size = LFH_huge_alloc_size(old_class_size);
+    new_class_size = LFH_huge_alloc_size(new_class_size);
+    if (!new_class && !old_class && old_class_size == new_class_size)
+        goto in_place;
+
+    if (flags & HEAP_REALLOC_IN_PLACE_ONLY)
+        return NULL;
+
+    if (!(new_ptr = LFH_allocate(flags, new_size)))
+        return NULL;
+
+    memcpy(new_ptr, old_ptr, old_size);
+
+    if (LFH_free(flags, old_ptr))
+        return new_ptr;
+
+    LFH_free(flags, new_ptr);
+    return NULL;
+
+in_place:
+    LFH_block_initialize(block, flags, old_size, new_size, old_class_size);
+    return old_ptr;
+}
+
+static inline size_t LFH_get_allocated_size(ULONG flags, const LFH_ptr *ptr)
+{
+    const LFH_block *block = LFH_block_from_ptr(ptr);
+    return LFH_block_get_alloc_size(block, flags);
+}
+
+static inline BOOLEAN LFH_validate(ULONG flags, const LFH_ptr *ptr)
+{
+    const LFH_block *block = LFH_block_from_ptr(ptr);
+    const LFH_heap *heap;
+
+    /* clear HEAP_VALIDATE so we only validate block */
+    if (ptr)
+        return LFH_validate_used_block(flags & ~HEAP_VALIDATE, block);
+
+    if (!(heap = LFH_thread_heap(FALSE)))
+        return TRUE;
+
+    return LFH_validate_heap(flags, heap);
+}
+
+static inline void LFH_get_user_info( ULONG flags, void *ptr, void **user_value, ULONG *user_flags )
+{
+    LFH_block *block = LFH_block_from_ptr(ptr);
+    void **tmp;
+
+    if (!(*user_flags = HEAP_USER_FLAGS( block->flags ))) return;
+
+    tmp = (void **)((char *)block + LFH_block_get_class_size( block ) - ALIGNMENT);
+    *user_flags = *user_flags & ~HEAP_ADD_USER_INFO;
+    *user_value = tmp[1];
+}
+
+static inline void LFH_set_user_value( ULONG flags, void *ptr, void *user_value )
+{
+    LFH_block *block = LFH_block_from_ptr(ptr);
+    void **tmp;
+
+    if (!(block->flags & BLOCK_FLAG_USER_INFO)) return;
+
+    tmp = (void **)((char *)block + LFH_block_get_class_size( block ) - ALIGNMENT);
+    tmp[1] = user_value;
+}
+
+static inline void LFH_set_user_flags( ULONG flags, void *ptr, ULONG clear, ULONG set )
+{
+    LFH_block *block = LFH_block_from_ptr(ptr);
+
+    if (!(block->flags & BLOCK_FLAG_USER_INFO)) return;
+
+    block->flags &= ~BLOCK_USER_FLAGS( clear );
+    block->flags |= BLOCK_USER_FLAGS( set );
+}
+
+static inline BOOLEAN LFH_try_validate_all(ULONG flags)
+{
+    if (!(flags & HEAP_VALIDATE_ALL))
+        return TRUE;
+
+    if (LFH_validate(flags, NULL))
+        return TRUE;
+
+    LFH_dump_heap(LFH_thread_heap(FALSE));
+    return FALSE;
+}
+
+NTSTATUS HEAP_lfh_allocate(HANDLE heap, ULONG flags, SIZE_T size, void **out)
+{
+    TRACE("heap %p, flags %08lx, size %Ix, out %p.\n", heap, flags, size, out);
+
+    if (!LFH_try_validate_all(flags))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!(*out = LFH_allocate(flags, size)))
+        return STATUS_NO_MEMORY;
+
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_free(HANDLE heap, ULONG flags, void *ptr)
+{
+    TRACE("heap %p, flags %08lx, ptr %p.\n", heap, flags, ptr);
+
+    if (!LFH_try_validate_all(flags))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!LFH_validate(flags, ptr))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!LFH_free(flags, ptr))
+        return STATUS_INVALID_PARAMETER;
+
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_reallocate(HANDLE heap, ULONG flags, void *ptr, SIZE_T size, void **out)
+{
+    TRACE("heap %p, flags %08lx, ptr %p, size %Ix, out %p.\n", heap, flags, ptr, size, out);
+
+    if (!LFH_try_validate_all(flags))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!LFH_validate(flags, ptr))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!(*out = LFH_reallocate(flags, ptr, size)))
+        return STATUS_NO_MEMORY;
+
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_get_allocated_size(HANDLE heap, ULONG flags, const void *ptr, SIZE_T* out)
+{
+    TRACE("heap %p, flags %08lx, ptr %p, out %p.\n", heap, flags, ptr, out);
+
+    if (!LFH_try_validate_all(flags))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!LFH_validate(flags, ptr))
+        return STATUS_INVALID_PARAMETER;
+
+    *out = LFH_get_allocated_size(flags, ptr);
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_validate(HANDLE heap, ULONG flags, const void *ptr)
+{
+    TRACE("heap %p, flags %08lx, ptr %p.\n", heap, flags, ptr);
+
+    if (!LFH_try_validate_all(flags))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!LFH_validate(flags, ptr))
+        return STATUS_INVALID_PARAMETER;
+
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_get_user_info( HANDLE heap, ULONG flags, void *ptr, void **user_value, ULONG *user_flags )
+{
+    TRACE("heap %p, flags %08lx, ptr %p, user_value %p, user_flags %p.\n", heap, flags, ptr, user_value, user_flags);
+
+    if (!LFH_try_validate_all(flags))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!LFH_validate(flags, ptr))
+        return STATUS_INVALID_PARAMETER;
+
+    LFH_get_user_info(flags, ptr, user_value, user_flags);
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_set_user_value( HANDLE heap, ULONG flags, void *ptr, void *user_value )
+{
+    TRACE("heap %p, flags %08lx, ptr %p, user_value %p.\n", heap, flags, ptr, user_value);
+
+    if (!LFH_try_validate_all(flags))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!LFH_validate(flags, ptr))
+        return STATUS_INVALID_PARAMETER;
+
+    LFH_set_user_value(flags, ptr, user_value);
+    return STATUS_SUCCESS;
+}
+
+NTSTATUS HEAP_lfh_set_user_flags( HANDLE heap, ULONG flags, void *ptr, ULONG clear, ULONG set )
+{
+    TRACE("heap %p, flags %08lx, ptr %p, clear %08lx, set %08lx.\n", heap, flags, ptr, clear, set);
+
+    if (!LFH_try_validate_all(flags))
+        return STATUS_INVALID_PARAMETER;
+
+    if (!LFH_validate(flags, ptr))
+        return STATUS_INVALID_PARAMETER;
+
+    LFH_set_user_flags(flags, ptr, clear, set);
+    return STATUS_SUCCESS;
+}
+
+void HEAP_lfh_notify_thread_destroy(BOOLEAN last)
+{
+    SLIST_HEADER *list_orphan = LFH_orphan_list();
+    SLIST_ENTRY *entry_orphan = NULL;
+    LFH_heap *heap;
+
+    if (last)
+    {
+        while ((entry_orphan || (entry_orphan = RtlInterlockedFlushSList(list_orphan))))
+        {
+            LFH_heap *orphan = LIST_ENTRY(entry_orphan, LFH_heap, entry_orphan);
+            entry_orphan = entry_orphan->Next;
+            LFH_heap_finalize(orphan);
+        }
+        LFH_memory_deallocate(list_orphan, BLOCK_ARENA_SIZE);
+    }
+    else if ((heap = LFH_thread_heap(FALSE)) && LFH_validate_heap(0, heap))
+        RtlInterlockedPushEntrySList(list_orphan, &heap->entry_orphan);
+}
+
+void HEAP_lfh_set_debug_flags(ULONG flags)
+{
+    LFH_heap *heap = LFH_thread_heap(FALSE);
+    if (!heap) return;
+
+    LFH_deallocate_deferred_blocks(heap);
+    LFH_deallocated_cached_arenas(heap);
+}
diff --git a/dlls/ntdll/ntdll_misc.h b/dlls/ntdll/ntdll_misc.h
index 11111111111..11111111111 100644
--- a/dlls/ntdll/ntdll_misc.h
+++ b/dlls/ntdll/ntdll_misc.h
@@ -155,7 +155,32 @@ static inline TEB64 *NtCurrentTeb64(void) { return NULL; }
 static inline TEB64 *NtCurrentTeb64(void) { return (TEB64 *)NtCurrentTeb()->GdiBatchCount; }
 #endif
 
+#define HEAP_STD 0
+#define HEAP_LAL 1
+#define HEAP_LFH 2
+
+/* some undocumented flags (names are made up) */
+#define HEAP_PRIVATE          0x00001000
+#define HEAP_ADD_USER_INFO    0x00000100
+#define HEAP_USER_FLAGS_MASK  0x00000f00
+#define HEAP_PAGE_ALLOCS      0x01000000
+#define HEAP_VALIDATE         0x10000000
+#define HEAP_VALIDATE_ALL     0x20000000
+#define HEAP_VALIDATE_PARAMS  0x40000000
+#define HEAP_CHECKING_ENABLED 0x80000000
+
+NTSTATUS HEAP_lfh_allocate( HANDLE std_heap, ULONG flags, SIZE_T size, void **out );
+NTSTATUS HEAP_lfh_free( HANDLE std_heap, ULONG flags, void *ptr );
+NTSTATUS HEAP_lfh_reallocate( HANDLE std_heap, ULONG flags, void *ptr, SIZE_T size, void **out );
+NTSTATUS HEAP_lfh_get_allocated_size( HANDLE std_heap, ULONG flags, const void *ptr, SIZE_T *out );
+NTSTATUS HEAP_lfh_validate( HANDLE std_heap, ULONG flags, const void *ptr );
+NTSTATUS HEAP_lfh_get_user_info( HANDLE std_handle, ULONG flags, void *ptr, void **user_value, ULONG *user_flags );
+NTSTATUS HEAP_lfh_set_user_value( HANDLE std_handle, ULONG flags, void *ptr, void *user_value );
+NTSTATUS HEAP_lfh_set_user_flags( HANDLE std_handle, ULONG flags, void *ptr, ULONG clear, ULONG set );
+
 void HEAP_notify_thread_destroy( BOOLEAN last );
+void HEAP_lfh_notify_thread_destroy( BOOLEAN last );
+void HEAP_lfh_set_debug_flags( ULONG flags );
 
 #define HASH_STRING_ALGORITHM_DEFAULT  0
 #define HASH_STRING_ALGORITHM_X65599   1
diff --git a/dlls/ntdll/unix/unix_private.h b/dlls/ntdll/unix/unix_private.h
index 11111111111..11111111111 100644
--- a/dlls/ntdll/unix/unix_private.h
+++ b/dlls/ntdll/unix/unix_private.h
@@ -69,6 +69,7 @@ struct ntdll_thread_data
     void              *param;         /* thread entry point parameter */
     void              *jmp_buf;       /* setjmp buffer for exception handling */
     unsigned int       fast_alert_obj; /* linux object for the fast alert event */
+    void              *heap;          /* thread local heap data */
 };
 
 C_ASSERT( sizeof(struct ntdll_thread_data) <= sizeof(((TEB *)0)->GdiTebBatch) );
-- 
2.39.0

