From e4716a6d3f7f33dfe195012e1ca2866139047338 Mon Sep 17 00:00:00 2001
From: Zebediah Figura <z.figura12@gmail.com>
Date: Tue, 6 Apr 2021 15:37:02 -0500
Subject: [PATCH 25/31] ntdll: Use fast synchronization objects.

---
 dlls/ntdll/unix/sync.c | 898 +++++++++++++++++++++++++++++++++++++++++
 1 file changed, 898 insertions(+)

diff --git a/dlls/ntdll/unix/sync.c b/dlls/ntdll/unix/sync.c
index 8c772d15108..84b62189ddc 100644
--- a/dlls/ntdll/unix/sync.c
+++ b/dlls/ntdll/unix/sync.c
@@ -30,9 +30,11 @@
 #include <assert.h>
 #include <errno.h>
 #include <fcntl.h>
+#include <inttypes.h>
 #include <limits.h>
 #include <signal.h>
 #include <sys/types.h>
+#include <sys/ioctl.h>
 #include <sys/mman.h>
 #ifdef HAVE_SYS_SYSCALL_H
 #include <sys/syscall.h>
@@ -49,6 +53,7 @@
 #endif
 #include <string.h>
 #include <stdarg.h>
+#include <stdint.h>
 #include <stdio.h>
 #include <stdlib.h>
 #include <time.h>
@@ -58,6 +63,9 @@
 # include <mach/semaphore.h>
 # include <mach/mach_time.h>
 #endif
+#ifdef HAVE_LINUX_WINESYNC_H
+# include <linux/winesync.h>
+#endif
 
 #include "ntstatus.h"
 #define WIN32_NO_STATUS
@@ -254,6 +262,853 @@ static NTSTATUS validate_open_object_attributes( const OBJECT_ATTRIBUTES *attr )
 }
 
 
+#ifdef HAVE_LINUX_WINESYNC_H
+
+/* glibc passes the sigset pointer directly to the linux kernel, but defines
+ * sigset_t to be larger. Manually define the kernel sigset size here. */
+#define KERNEL_SIGSET_SIZE (64 / 8) /* 64 signals / 8 bits per byte */
+
+struct timespec64
+{
+    long long tv_sec;
+    long long tv_nsec;
+};
+
+static int get_fast_sync_device(void)
+{
+    static int fast_sync_fd = -2;
+
+    if (fast_sync_fd == -2)
+    {
+        HANDLE device;
+        int fd, needs_close;
+        NTSTATUS ret;
+
+        SERVER_START_REQ( get_fast_sync_device )
+        {
+            if (!(ret = wine_server_call( req ))) device = wine_server_ptr_handle( reply->handle );
+        }
+        SERVER_END_REQ;
+
+        if (!ret)
+        {
+            if (!server_get_unix_fd( device, 0, &fd, &needs_close, NULL, NULL ))
+            {
+                if (InterlockedCompareExchange( &fast_sync_fd, fd, -2 ) != -2)
+                {
+                    /* someone beat us to it */
+                    if (needs_close) close( fd );
+                    NtClose( device );
+                }
+                /* otherwise don't close the device */
+            }
+            else
+            {
+                InterlockedCompareExchange( &fast_sync_fd, -1, -2 );
+                NtClose( device );
+            }
+        }
+        else
+        {
+            InterlockedCompareExchange( &fast_sync_fd, -1, -2 );
+        }
+    }
+    return fast_sync_fd;
+}
+
+/* It's possible for synchronization primitives to remain alive even after being
+ * closed, because a thread is still waiting on them. It's rare in practice, and
+ * documented as being undefined behaviour by Microsoft, but it works, and some
+ * applications rely on it. This means we need to refcount handles, and defer
+ * deleting them on the server side until the refcount reaches zero. We do this
+ * by having each client process hold a handle to the fast synchronization
+ * object, as well as a private refcount. When the client refcount reaches zero,
+ * it closes the handle; when all handles are closed, the server deletes the
+ * fast synchronization object.
+ *
+ * We want lookup of objects from the cache to be very fast; ideally, it should
+ * be lock-free. We achieve this by using atomic modifications to "refcount",
+ * and guaranteeing that all other fields are valid and correct *as long as*
+ * refcount is nonzero, and we store the entire structure in memory which will
+ * never be freed.
+ *
+ * This means that acquiring the object can't use a simple atomic increment; it
+ * has to use a compare-and-swap loop to ensure that it doesn't try to increment
+ * an object with a zero refcount. That's still leagues better than a real lock,
+ * though, and release can be a single atomic decrement.
+ *
+ * It also means that threads modifying the cache need to take a lock, to
+ * prevent other threads from writing to it concurrently.
+ *
+ * It's possible for an object currently in use (by a waiter) to be closed and
+ * the same handle immediately reallocated to a different object. This should be
+ * a very rare situation, and in that case we simply don't cache the handle.
+ */
+struct fast_sync_cache_entry
+{
+    LONG refcount;
+    unsigned int obj;
+    enum fast_sync_type type;
+    unsigned int access;
+    BOOL closed;
+    /* handle to the underlying fast sync object, stored as obj_handle_t to save
+     * space */
+    obj_handle_t handle;
+};
+
+
+static void release_fast_sync_obj( struct fast_sync_cache_entry *cache )
+{
+    /* save the handle now; as soon as the refcount hits 0 we cannot access the
+     * cache anymore */
+    HANDLE handle = wine_server_ptr_handle( cache->handle );
+    LONG refcount = InterlockedDecrement( &cache->refcount );
+
+    assert( refcount >= 0 );
+
+    if (!refcount)
+    {
+        NTSTATUS ret = NtClose( handle );
+        assert( !ret );
+    }
+}
+
+
+static BOOL fast_sync_types_match( enum fast_sync_type a, enum fast_sync_type b )
+{
+    if (a == b) return TRUE;
+    if (a == FAST_SYNC_AUTO_EVENT && b == FAST_SYNC_MANUAL_EVENT) return TRUE;
+    if (b == FAST_SYNC_AUTO_EVENT && a == FAST_SYNC_MANUAL_EVENT) return TRUE;
+    return FALSE;
+}
+
+
+/* returns a pointer to a cache entry; if the object could not be cached,
+ * returns "stack_cache" instead, which should be allocated on stack */
+static NTSTATUS get_fast_sync_obj( HANDLE handle, enum fast_sync_type desired_type, ACCESS_MASK desired_access,
+                                   struct fast_sync_cache_entry *stack_cache,
+                                   struct fast_sync_cache_entry **ret_cache )
+{
+    struct fast_sync_cache_entry *cache = stack_cache;
+    NTSTATUS ret;
+
+    *ret_cache = stack_cache;
+
+    SERVER_START_REQ( get_fast_sync_obj )
+    {
+        req->handle = wine_server_obj_handle( handle );
+        if (!(ret = wine_server_call( req )))
+        {
+            cache->handle = reply->handle;
+            cache->access = reply->access;
+            cache->type = reply->type;
+            cache->obj = reply->obj;
+            cache->refcount = 1;
+            cache->closed = FALSE;
+        }
+    }
+    SERVER_END_REQ;
+
+    if (!ret && desired_type && !fast_sync_types_match( cache->type, desired_type ))
+    {
+        release_fast_sync_obj( cache );
+        return STATUS_OBJECT_TYPE_MISMATCH;
+    }
+
+    if (!ret && (cache->access & desired_access) != desired_access)
+    {
+        release_fast_sync_obj( cache );
+        return STATUS_ACCESS_DENIED;
+    }
+
+    return ret;
+}
+
+
+static NTSTATUS fast_release_semaphore_obj( int device, unsigned int obj, ULONG count, ULONG *prev_count )
+{
+    struct winesync_sem_args args = {0};
+    NTSTATUS ret;
+
+    args.sem = obj;
+    args.count = count;
+    ret = ioctl( device, WINESYNC_IOC_PUT_SEM, &args );
+    if (ret < 0)
+    {
+        if (errno == EOVERFLOW)
+            return STATUS_SEMAPHORE_LIMIT_EXCEEDED;
+        else
+            return errno_to_status( errno );
+    }
+    if (prev_count) *prev_count = args.count;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS fast_release_semaphore( HANDLE handle, ULONG count, ULONG *prev_count )
+{
+    struct fast_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_fast_sync_obj( handle, FAST_SYNC_SEMAPHORE,
+                                  SEMAPHORE_MODIFY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = fast_release_semaphore_obj( device, cache->obj, count, prev_count );
+
+    release_fast_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS fast_query_semaphore_obj( int device, unsigned int obj, SEMAPHORE_BASIC_INFORMATION *info )
+{
+    struct winesync_sem_args args = {0};
+    NTSTATUS ret;
+
+    args.sem = obj;
+    ret = ioctl( device, WINESYNC_IOC_READ_SEM, &args );
+
+    if (ret < 0)
+        return errno_to_status( errno );
+    info->CurrentCount = args.count;
+    info->MaximumCount = args.max;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS fast_query_semaphore( HANDLE handle, SEMAPHORE_BASIC_INFORMATION *info )
+{
+    struct fast_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_fast_sync_obj( handle, FAST_SYNC_SEMAPHORE,
+                                  SEMAPHORE_QUERY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = fast_query_semaphore_obj( device, cache->obj, info );
+
+    release_fast_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS fast_set_event_obj( int device, unsigned int obj, LONG *prev_state )
+{
+    struct winesync_sem_args args = {0};
+    NTSTATUS ret;
+
+    args.sem = obj;
+    args.count = 1;
+    ret = ioctl( device, WINESYNC_IOC_PUT_SEM, &args );
+    if (ret < 0)
+    {
+        if (errno == EOVERFLOW)
+        {
+            if (prev_state) *prev_state = 1;
+            return STATUS_SUCCESS;
+        }
+        else
+            return errno_to_status( errno );
+    }
+    if (prev_state) *prev_state = 0;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS fast_set_event( HANDLE handle, LONG *prev_state )
+{
+    struct fast_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_fast_sync_obj( handle, FAST_SYNC_AUTO_EVENT,
+                                  EVENT_MODIFY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = fast_set_event_obj( device, cache->obj, prev_state );
+
+    release_fast_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS fast_reset_event_obj( int device, unsigned int obj, LONG *prev_state )
+{
+    static const struct timespec64 timespec;
+    struct winesync_wait_args args = {0};
+    struct winesync_wait_obj wait_obj;
+    NTSTATUS ret;
+
+    args.timeout = (uintptr_t)&timespec;
+    args.objs = (uintptr_t)&wait_obj;
+    args.count = 1;
+    args.owner = GetCurrentThreadId();
+
+    wait_obj.obj = obj;
+    wait_obj.flags = WINESYNC_WAIT_FLAG_GET;
+
+    ret = ioctl( device, WINESYNC_IOC_WAIT_ANY, &args );
+    if (ret < 0)
+    {
+        if (errno == ETIMEDOUT)
+        {
+            if (prev_state) *prev_state = 0;
+            return STATUS_SUCCESS;
+        }
+        else
+            return errno_to_status( errno );
+    }
+    if (prev_state) *prev_state = 1;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS fast_reset_event( HANDLE handle, LONG *prev_state )
+{
+    struct fast_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_fast_sync_obj( handle, FAST_SYNC_AUTO_EVENT,
+                                  EVENT_MODIFY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = fast_reset_event_obj( device, cache->obj, prev_state );
+
+    release_fast_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS fast_pulse_event_obj( int device, unsigned int obj, LONG *prev_state )
+{
+    struct winesync_sem_args args = {0};
+    NTSTATUS ret;
+
+    args.sem = obj;
+    args.count = 1;
+    ret = ioctl( device, WINESYNC_IOC_PULSE_SEM, &args );
+    if (ret < 0)
+    {
+        if (errno == EOVERFLOW)
+        {
+            if (prev_state) *prev_state = 1;
+            return STATUS_SUCCESS;
+        }
+        else
+            return errno_to_status( errno );
+    }
+    if (prev_state) *prev_state = 0;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS fast_pulse_event( HANDLE handle, LONG *prev_state )
+{
+    struct fast_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_fast_sync_obj( handle, FAST_SYNC_AUTO_EVENT,
+                                  EVENT_MODIFY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = fast_pulse_event_obj( device, cache->obj, prev_state );
+
+    release_fast_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS fast_query_event_obj( int device, unsigned int obj, enum fast_sync_type type, EVENT_BASIC_INFORMATION *info )
+{
+    struct winesync_sem_args args = {0};
+    NTSTATUS ret;
+
+    args.sem = obj;
+    ret = ioctl( device, WINESYNC_IOC_READ_SEM, &args );
+
+    if (ret < 0)
+        return errno_to_status( errno );
+    info->EventType = (type == FAST_SYNC_AUTO_EVENT) ? SynchronizationEvent : NotificationEvent;
+    info->EventState = args.count;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS fast_query_event( HANDLE handle, EVENT_BASIC_INFORMATION *info )
+{
+    struct fast_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_fast_sync_obj( handle, FAST_SYNC_AUTO_EVENT,
+                                  EVENT_QUERY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = fast_query_event_obj( device, cache->obj, cache->type, info );
+
+    release_fast_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS fast_release_mutex_obj( int device, unsigned int obj, LONG *prev_count )
+{
+    struct winesync_mutex_args args = {0};
+    NTSTATUS ret;
+
+    args.mutex = obj;
+    args.owner = GetCurrentThreadId();
+    ret = ioctl( device, WINESYNC_IOC_PUT_MUTEX, &args );
+
+    if (ret < 0)
+    {
+        if (errno == EOVERFLOW)
+            return STATUS_MUTANT_LIMIT_EXCEEDED;
+        else if (errno == EPERM)
+            return STATUS_MUTANT_NOT_OWNED;
+        else
+            return errno_to_status( errno );
+    }
+    if (prev_count) *prev_count = 1 - args.count;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS fast_release_mutex( HANDLE handle, LONG *prev_count )
+{
+    struct fast_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_fast_sync_obj( handle, FAST_SYNC_MUTEX, 0, &stack_cache, &cache )))
+        return ret;
+
+    ret = fast_release_mutex_obj( device, cache->obj, prev_count );
+
+    release_fast_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS fast_query_mutex_obj( int device, unsigned int obj, MUTANT_BASIC_INFORMATION *info )
+{
+    struct winesync_mutex_args args = {0};
+    NTSTATUS ret;
+
+    args.mutex = obj;
+    ret = ioctl( device, WINESYNC_IOC_READ_MUTEX, &args );
+
+    if (ret < 0)
+    {
+        if (errno == EOWNERDEAD)
+        {
+            info->AbandonedState = TRUE;
+            info->OwnedByCaller = FALSE;
+            info->CurrentCount = 1;
+            return STATUS_SUCCESS;
+        }
+        else
+            return errno_to_status( errno );
+    }
+    info->AbandonedState = FALSE;
+    info->OwnedByCaller = (args.owner == GetCurrentThreadId());
+    info->CurrentCount = 1 - args.count;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS fast_query_mutex( HANDLE handle, MUTANT_BASIC_INFORMATION *info )
+{
+    struct fast_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_fast_sync_obj( handle, FAST_SYNC_MUTEX, MUTANT_QUERY_STATE,
+                                  &stack_cache, &cache )))
+        return ret;
+
+    ret = fast_query_mutex_obj( device, cache->obj, info );
+
+    release_fast_sync_obj( cache );
+    return ret;
+}
+
+static void timespec64_from_timeout( struct timespec64 *timespec, const LARGE_INTEGER *timeout )
+{
+    struct timespec now;
+    timeout_t relative;
+
+    clock_gettime( CLOCK_MONOTONIC, &now );
+
+    if (timeout->QuadPart <= 0)
+    {
+        relative = -timeout->QuadPart;
+    }
+    else
+    {
+        LARGE_INTEGER system_now;
+
+        /* the system clock is probably REALTIME, so we need to convert to
+         * relative time first */
+        NtQuerySystemTime( &system_now );
+        relative = timeout->QuadPart - system_now.QuadPart;
+    }
+
+    timespec->tv_sec = now.tv_sec + (relative / TICKSPERSEC);
+    timespec->tv_nsec = now.tv_nsec + ((relative % TICKSPERSEC) * 100);
+    if (timespec->tv_nsec >= 1000000000)
+    {
+        timespec->tv_nsec -= 1000000000;
+        ++timespec->tv_sec;
+    }
+}
+
+static void select_queue( HANDLE queue )
+{
+    SERVER_START_REQ( fast_select_queue )
+    {
+        req->handle = wine_server_obj_handle( queue );
+        wine_server_call( req );
+    }
+    SERVER_END_REQ;
+}
+
+static void unselect_queue( HANDLE queue, BOOL signaled )
+{
+    SERVER_START_REQ( fast_unselect_queue )
+    {
+        req->handle = wine_server_obj_handle( queue );
+        req->signaled = signaled;
+        wine_server_call( req );
+    }
+    SERVER_END_REQ;
+}
+
+static NTSTATUS fast_wait_objs( int device, DWORD count, const struct winesync_wait_obj *objs,
+                                BOOLEAN wait_any, BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    struct winesync_wait_args args = {0};
+    struct timespec64 timespec;
+    uintptr_t timeout_ptr = 0;
+    unsigned long request;
+    int ret;
+
+    if (timeout && timeout->QuadPart != TIMEOUT_INFINITE)
+    {
+        timeout_ptr = (uintptr_t)&timespec;
+        timespec64_from_timeout( &timespec, timeout );
+    }
+    args.objs = (uintptr_t)objs;
+    args.count = count;
+    args.owner = GetCurrentThreadId();
+    args.index = ~0u;
+
+    if (wait_any || count == 1)
+        request = WINESYNC_IOC_WAIT_ANY;
+    else
+        request = WINESYNC_IOC_WAIT_ALL;
+
+    if (alertable)
+    {
+        static const struct timespec64 zero_timespec;
+
+        /* if there is an already signaled object and an APC available, the
+         * object is returned first */
+        args.timeout = (uintptr_t)&zero_timespec;
+        do
+        {
+            ret = ioctl( device, request, &args );
+        } while (ret < 0 && errno == EINTR);
+
+        if (ret < 0 && errno == ETIMEDOUT)
+        {
+            sigset_t old_set;
+
+            /* We need to mask SIGUSR1 here, to avoid a race where the signal
+             * arrives after we check for user APCs but before we perform the
+             * wait ioctl. However, we need SIGUSR1 to be unmasked while the
+             * wait ioctl is in progress. We use the "sigmask" field, analogous
+             * to the similar argument to pselect(2), to achieve this. */
+
+            pthread_sigmask( SIG_BLOCK, &server_block_set, &old_set );
+
+            args.timeout = timeout_ptr;
+
+            args.sigmask = (uintptr_t)&old_set;
+            args.sigsetsize = KERNEL_SIGSET_SIZE;
+
+            do
+            {
+                static const LARGE_INTEGER timeout;
+                NTSTATUS status;
+
+                status = server_wait( NULL, 0, SELECT_INTERRUPTIBLE | SELECT_ALERTABLE, &timeout );
+                if (status == STATUS_USER_APC)
+                {
+                    pthread_sigmask( SIG_SETMASK, &old_set, NULL );
+                    return STATUS_USER_APC;
+                }
+
+                ret = ioctl( device, request, &args );
+            } while (ret < 0 && errno == EINTR);
+
+            pthread_sigmask( SIG_SETMASK, &old_set, NULL );
+        }
+    }
+    else
+    {
+        args.timeout = timeout_ptr;
+        do
+        {
+            ret = ioctl( device, request, &args );
+        } while (ret < 0 && errno == EINTR);
+    }
+
+    if (!ret)
+        return wait_any ? args.index : 0;
+    else if (errno == EOWNERDEAD)
+        return STATUS_ABANDONED + (wait_any ? args.index : 0);
+    else if (errno == ETIMEDOUT)
+        return STATUS_TIMEOUT;
+    else
+        return errno_to_status( errno );
+}
+
+static void fill_winesync_wait_obj( struct winesync_wait_obj *wait_obj, const struct fast_sync_cache_entry *cache )
+{
+    wait_obj->obj = cache->obj;
+    wait_obj->flags = 0;
+    switch (cache->type)
+    {
+        case FAST_SYNC_AUTO_EVENT:
+        case FAST_SYNC_AUTO_SERVER:
+        case FAST_SYNC_MUTEX:
+        case FAST_SYNC_SEMAPHORE:
+            wait_obj->flags |= WINESYNC_WAIT_FLAG_GET;
+            break;
+
+        case FAST_SYNC_MANUAL_EVENT:
+        case FAST_SYNC_MANUAL_SERVER:
+        case FAST_SYNC_QUEUE:
+            break;
+    }
+}
+
+static NTSTATUS fast_wait( DWORD count, const HANDLE *handles, BOOLEAN wait_any,
+                           BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    struct fast_sync_cache_entry stack_cache[64], *cache[64];
+    struct winesync_wait_obj objs[64];
+    HANDLE queue = NULL;
+    NTSTATUS ret;
+    DWORD i, j;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    for (i = 0; i < count; ++i)
+    {
+        if ((ret = get_fast_sync_obj( handles[i], 0, SYNCHRONIZE, &stack_cache[i], &cache[i] )))
+        {
+            for (j = 0; j < i; ++j)
+                release_fast_sync_obj( cache[j] );
+            return ret;
+        }
+        if (cache[i]->type == FAST_SYNC_QUEUE)
+            queue = handles[i];
+
+        fill_winesync_wait_obj( &objs[i], cache[i] );
+    }
+
+    if (queue) select_queue( queue );
+
+    ret = fast_wait_objs( device, count, objs, wait_any, alertable, timeout );
+
+    if (queue) unselect_queue( queue, handles[ret] == queue );
+
+    for (i = 0; i < count; ++i)
+        release_fast_sync_obj( cache[i] );
+
+    return ret;
+}
+
+static NTSTATUS fast_signal_and_wait( HANDLE signal, HANDLE wait,
+                                      BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    struct fast_sync_cache_entry signal_stack_cache, *signal_cache;
+    struct fast_sync_cache_entry wait_stack_cache, *wait_cache;
+    HANDLE queue = NULL;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_fast_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_fast_sync_obj( signal, 0, 0, &signal_stack_cache, &signal_cache )))
+        return ret;
+
+    switch (signal_cache->type)
+    {
+        case FAST_SYNC_SEMAPHORE:
+            if (!(signal_cache->access & SEMAPHORE_MODIFY_STATE))
+            {
+                release_fast_sync_obj( signal_cache );
+                return STATUS_ACCESS_DENIED;
+            }
+            break;
+
+        case FAST_SYNC_AUTO_EVENT:
+        case FAST_SYNC_MANUAL_EVENT:
+            if (!(signal_cache->access & EVENT_MODIFY_STATE))
+            {
+                release_fast_sync_obj( signal_cache );
+                return STATUS_ACCESS_DENIED;
+            }
+            break;
+
+        case FAST_SYNC_MUTEX:
+            break;
+
+        default:
+            /* can't be signaled */
+            release_fast_sync_obj( signal_cache );
+            return STATUS_OBJECT_TYPE_MISMATCH;
+    }
+
+    if ((ret = get_fast_sync_obj( wait, 0, SYNCHRONIZE, &wait_stack_cache, &wait_cache )))
+    {
+        release_fast_sync_obj( signal_cache );
+        return ret;
+    }
+
+    if (wait_cache->type == FAST_SYNC_QUEUE)
+        queue = wait;
+
+    switch (signal_cache->type)
+    {
+        case FAST_SYNC_SEMAPHORE:
+            ret = fast_release_semaphore_obj( device, signal_cache->obj, 1, NULL );
+            break;
+
+        case FAST_SYNC_AUTO_EVENT:
+        case FAST_SYNC_MANUAL_EVENT:
+            ret = fast_set_event_obj( device, signal_cache->obj, NULL );
+            break;
+
+        case FAST_SYNC_MUTEX:
+            ret = fast_release_mutex_obj( device, signal_cache->obj, NULL );
+            break;
+
+        default:
+            assert( 0 );
+            break;
+    }
+
+    if (!ret)
+    {
+        struct winesync_wait_obj wait_obj;
+
+        fill_winesync_wait_obj( &wait_obj, wait_cache );
+
+        if (queue) select_queue( queue );
+
+        ret = fast_wait_objs( device, 1, &wait_obj, TRUE, alertable, timeout );
+
+        if (queue) unselect_queue( queue, !ret );
+    }
+
+    release_fast_sync_obj( signal_cache );
+    release_fast_sync_obj( wait_cache );
+    return ret;
+}
+
+#else
+
+static NTSTATUS fast_release_semaphore( HANDLE handle, ULONG count, ULONG *prev_count )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS fast_query_semaphore( HANDLE handle, SEMAPHORE_BASIC_INFORMATION *info )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS fast_set_event( HANDLE handle, LONG *prev_state )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS fast_reset_event( HANDLE handle, LONG *prev_state )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS fast_pulse_event( HANDLE handle, LONG *prev_state )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS fast_query_event( HANDLE handle, EVENT_BASIC_INFORMATION *info )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS fast_release_mutex( HANDLE handle, LONG *prev_count )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS fast_query_mutex( HANDLE handle, MUTANT_BASIC_INFORMATION *info )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS fast_wait( DWORD count, const HANDLE *handles, BOOLEAN wait_any,
+                           BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS fast_signal_and_wait( HANDLE signal, HANDLE wait,
+                                      BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+#endif
+
+
 /******************************************************************************
  *              NtCreateSemaphore (NTDLL.@)
  */
@@ -333,6 +1188,12 @@ NTSTATUS WINAPI NtQuerySemaphore( HANDLE handle, SEMAPHORE_INFORMATION_CLASS cla
 
     if (len != sizeof(SEMAPHORE_BASIC_INFORMATION)) return STATUS_INFO_LENGTH_MISMATCH;
 
+    if ((ret = fast_query_semaphore( handle, out )) != STATUS_NOT_IMPLEMENTED)
+    {
+        if (!ret && ret_len) *ret_len = sizeof(SEMAPHORE_BASIC_INFORMATION);
+        return ret;
+    }
+
     SERVER_START_REQ( query_semaphore )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -357,6 +1218,9 @@ NTSTATUS WINAPI NtReleaseSemaphore( HANDLE handle, ULONG count, ULONG *previous
 
     TRACE( "handle %p, count %u, prev_count %p\n", handle, count, previous );
 
+    if ((ret = fast_release_semaphore( handle, count, previous )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( release_semaphore )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -440,6 +1304,9 @@ NTSTATUS WINAPI NtSetEvent( HANDLE handle, LONG *prev_state )
 
     TRACE( "handle %p, prev_state %p\n", handle, prev_state );
 
+    if ((ret = fast_set_event( handle, prev_state )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( event_op )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -461,6 +1328,9 @@ NTSTATUS WINAPI NtResetEvent( HANDLE handle, LONG *prev_state )
 
     TRACE( "handle %p, prev_state %p\n", handle, prev_state );
 
+    if ((ret = fast_reset_event( handle, prev_state )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( event_op )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -492,6 +1362,9 @@ NTSTATUS WINAPI NtPulseEvent( HANDLE handle, LONG *prev_state )
 
     TRACE( "handle %p, prev_state %p\n", handle, prev_state );
 
+    if ((ret = fast_pulse_event( handle, prev_state )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( event_op )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -524,6 +1397,12 @@ NTSTATUS WINAPI NtQueryEvent( HANDLE handle, EVENT_INFORMATION_CLASS class,
 
     if (len != sizeof(EVENT_BASIC_INFORMATION)) return STATUS_INFO_LENGTH_MISMATCH;
 
+    if ((ret = fast_query_event( handle, out )) != STATUS_NOT_IMPLEMENTED)
+    {
+        if (!ret && ret_len) *ret_len = sizeof(EVENT_BASIC_INFORMATION);
+        return ret;
+    }
+
     SERVER_START_REQ( query_event )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -606,6 +1485,9 @@ NTSTATUS WINAPI NtReleaseMutant( HANDLE handle, LONG *prev_count )
 
     TRACE( "handle %p, prev_count %p\n", handle, prev_count );
 
+    if ((ret = fast_release_mutex( handle, prev_count )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( release_mutex )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -636,6 +1518,12 @@ NTSTATUS WINAPI NtQueryMutant( HANDLE handle, MUTANT_INFORMATION_CLASS class,
 
     if (len != sizeof(MUTANT_BASIC_INFORMATION)) return STATUS_INFO_LENGTH_MISMATCH;
 
+    if ((ret = fast_query_mutex( handle, out )) != STATUS_NOT_IMPLEMENTED)
+    {
+        if (!ret && ret_len) *ret_len = sizeof(MUTANT_BASIC_INFORMATION);
+        return ret;
+    }
+
     SERVER_START_REQ( query_mutex )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -1449,6 +2337,12 @@ NTSTATUS WINAPI NtWaitForMultipleObjects( DWORD count, const HANDLE *handles, BO
         TRACE( "}, timeout %s\n", debugstr_timeout(timeout) );
     }
 
+    if ((ret = fast_wait( count, handles, wait_any, alertable, timeout )) != STATUS_NOT_IMPLEMENTED)
+    {
+        TRACE( "-> %#x\n", ret );
+        return ret;
+    }
+
     if (alertable) flags |= SELECT_ALERTABLE;
     select_op.wait.op = wait_any ? SELECT_WAIT : SELECT_WAIT_ALL;
     for (i = 0; i < count; i++) select_op.wait.handles[i] = wine_server_obj_handle( handles[i] );
@@ -1475,17 +2369,21 @@ NTSTATUS WINAPI NtSignalAndWaitForSingleObject( HANDLE signal, HANDLE wait,
 {
     select_op_t select_op;
     UINT flags = SELECT_INTERRUPTIBLE;
+    NTSTATUS ret;
 
     TRACE( "signal %p, wait %p, alertable %u, timeout %s\n", signal, wait, alertable, debugstr_timeout(timeout) );
 
     if (do_fsync())
         return fsync_signal_and_wait( signal, wait, alertable, timeout );
 
     if (do_esync())
         return esync_signal_and_wait( signal, wait, alertable, timeout );
 
     if (!signal) return STATUS_INVALID_HANDLE;
 
+    if ((ret = fast_signal_and_wait( signal, wait, alertable, timeout )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     if (alertable) flags |= SELECT_ALERTABLE;
     select_op.signal_and_wait.op = SELECT_SIGNAL_AND_WAIT;
     select_op.signal_and_wait.wait = wine_server_obj_handle( wait );
-- 
2.34.1

