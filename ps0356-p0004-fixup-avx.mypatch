From 6068f3907dfaa761cb3fd55e34455e7cc72eff2b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?R=C3=A9mi=20Bernon?= <rbernon@codeweavers.com>
Date: Thu, 11 Mar 2021 17:12:03 +0100
Subject: [PATCH 4/4] fixup avx

---
 dlls/msvcrt/string.c | 315 ++++++++++++++++++++++++++-----------------
 1 file changed, 189 insertions(+), 126 deletions(-)

diff --git a/dlls/msvcrt/string.c b/dlls/msvcrt/string.c
index ac53a5ff7cb..88666bb05a5 100644
--- a/dlls/msvcrt/string.c
+++ b/dlls/msvcrt/string.c
@@ -2475,11 +2475,13 @@ int __cdecl memcmp(const void *ptr1, const void *ptr2, size_t n)
 
 extern BOOL sse2_supported;
 extern BOOL avx_supported;
+#define likely(x) __builtin_expect(x, 1)
+#define unlikely(x) __builtin_expect(x, 0)
 
 #define MEMMOVE_START_FWD(step, size) \
                             do { \
                                 static const int z = 1, y = 0; \
-                                while (n >= size) { do
+                                while (unlikely(n >= size)) { do
 #define MEMMOVE_END_FWD(size)   \
                                 while(0); d += size; s += size; n -= size; } \
                             } while(0)
@@ -2487,39 +2489,39 @@ extern BOOL avx_supported;
 #define MEMMOVE_START_BWD(step, size) \
                             do { \
                                 static const int z = -1, y = -step; \
-                                while (n >= size) { do
+                                while (unlikely(n >= size)) { do
 #define MEMMOVE_END_BWD(size)   \
                                 while(0); d -= size; s -= size; n -= size; } \
                             } while(0)
 
 #define MEMSET_START(size) \
                             do { \
-                                while (n >= size) { do
+                                while (unlikely(n >= size)) { do
 #define MEMSET_END(size)   \
                                 while(0); d += size; n -= size; } \
                             } while(0)
 
 #define MEMMOVE_DECLARE(pfx, step, size, next, ...) \
-    static inline void *__cdecl memmove_ ## pfx ## _forward_ ## size(void *dst, char *d, const char *s, size_t n) \
+    static inline void *memmove_ ## pfx ## _forward_ ## size(void *dst, char *d, const char *s, size_t n) \
     { \
         MEMMOVE_START_FWD(step, size) \
         { __VA_ARGS__ } \
         MEMMOVE_END_FWD(size); \
-        if (!n) return dst; \
+        if (unlikely(!n)) return dst; \
         return memmove_ ## pfx ## _forward_ ## next(dst, d, s, n); \
     } \
     \
-    static inline void *__cdecl memmove_ ## pfx ## _backward_ ## size(void *dst, char *d, const char *s, size_t n) \
+    static inline void *memmove_ ## pfx ## _backward_ ## size(void *dst, char *d, const char *s, size_t n) \
     { \
         MEMMOVE_START_BWD(step, size) \
         { __VA_ARGS__ } \
         MEMMOVE_END_BWD(size); \
-        if (!n) return dst; \
+        if (unlikely(!n)) return dst; \
         return memmove_ ## pfx ## _backward_ ## next(dst, d, s, n); \
     }
 
 #define MEMSET_DECLARE(pfx, size, next, init, ...) \
-    static inline void *__cdecl memset_ ## pfx ## _ ## size(char *dst, char *d, int v, size_t n) \
+    static inline void *memset_ ## pfx ## _ ## size(char *dst, char *d, int v, size_t n) \
     { \
         uint8_t tmp8 = v; \
         uint16_t tmp16 = ((uint16_t)tmp8 << 8) | tmp8; \
@@ -2529,18 +2531,20 @@ extern BOOL avx_supported;
         MEMSET_START(size) \
         { __VA_ARGS__ } \
         MEMSET_END(size); \
-        if (!n) return dst; \
+        if (unlikely(!n)) return dst; \
         return memset_ ## pfx ## _ ## next(dst, d, v, n); \
     } \
 
-static inline void *__cdecl memmove_c_unaligned_32(char *d, const char *s, size_t n)
+static inline void *memmove_c_unaligned_32(char *d, const char *s, size_t n)
 {
+    uint64_t tmp0, tmp1, tmp2, tmp3;
+
     if (n > 16)
     {
-        uint64_t tmp0 = *(uint64_t *)s;
-        uint64_t tmp1 = *(uint64_t *)(s + 8);
-        uint64_t tmp2 = *(uint64_t *)(s + n - 16);
-        uint64_t tmp3 = *(uint64_t *)(s + n - 8);
+        tmp0 = *(uint64_t *)s;
+        tmp1 = *(uint64_t *)(s + 8);
+        tmp2 = *(uint64_t *)(s + n - 16);
+        tmp3 = *(uint64_t *)(s + n - 8);
         *(uint64_t *)d = tmp0;
         *(uint64_t *)(d + 8) = tmp1;
         *(uint64_t *)(d + n - 16) = tmp2;
@@ -2549,24 +2553,24 @@ static inline void *__cdecl memmove_c_unaligned_32(char *d, const char *s, size_
     }
     if (n >= 8)
     {
-        uint64_t tmp0 = *(uint64_t *)s;
-        uint64_t tmp1 = *(uint64_t *)(s + n - 8);
+        tmp0 = *(uint64_t *)s;
+        tmp1 = *(uint64_t *)(s + n - 8);
         *(uint64_t *)d = tmp0;
         *(uint64_t *)(d + n - 8) = tmp1;
         return d;
     }
     if (n >= 4)
     {
-        uint32_t tmp0 = *(uint32_t *)s;
-        uint32_t tmp1 = *(uint32_t *)(s + n - 4);
+        tmp0 = *(uint32_t *)s;
+        tmp1 = *(uint32_t *)(s + n - 4);
         *(uint32_t *)d = tmp0;
         *(uint32_t *)(d + n - 4) = tmp1;
         return d;
     }
     if (n >= 2)
     {
-        uint16_t tmp0 = *(uint16_t *)s;
-        uint16_t tmp1 = *(uint16_t *)(s + n - 2);
+        tmp0 = *(uint16_t *)s;
+        tmp1 = *(uint16_t *)(s + n - 2);
         *(uint16_t *)d = tmp0;
         *(uint16_t *)(d + n - 2) = tmp1;
         return d;
@@ -2579,13 +2583,13 @@ static inline void *__cdecl memmove_c_unaligned_32(char *d, const char *s, size_
     return d;
 }
 
-static inline void *__cdecl memmove_c_forward_16(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_c_forward_16(void *dst, char *d, const char *s, size_t n)
 {
     memmove_c_unaligned_32(d, s, n);
     return dst;
 }
 
-static inline void *__cdecl memmove_c_backward_16(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_c_backward_16(void *dst, char *d, const char *s, size_t n)
 {
     return memmove_c_unaligned_32(d - n, s - n, n);
 }
@@ -2608,7 +2612,7 @@ MEMMOVE_DECLARE(c, 8, 64, 32,
     *(int64_t*)(d + 56 * z + y) = *(int64_t *)(s + 56 * z + y);
 )
 
-static inline void *__cdecl memset_c_unaligned_32(char *d, int v, size_t n)
+static inline void *memset_c_unaligned_32(char *d, int v, size_t n)
 {
     uint8_t tmp8 = v;
     uint16_t tmp16 = ((uint16_t)tmp8 << 8) | tmp8;
@@ -2649,7 +2653,7 @@ static inline void *__cdecl memset_c_unaligned_32(char *d, int v, size_t n)
     return d;
 }
 
-static inline void *__cdecl memset_c_16(void *dst, char *d, int v, size_t n)
+static inline void *memset_c_16(void *dst, char *d, int v, size_t n)
 {
     memset_c_unaligned_32(d, v, n);
     return dst;
@@ -2673,24 +2677,24 @@ MEMSET_DECLARE(c, 64, 32,,
     *(int64_t*)(d + 56) = tmp64;
 )
 
-static void *__cdecl memmove_c_forward(void *dst, char *d, const char *s, size_t n)
+static void *memmove_c_forward(void *dst, char *d, const char *s, size_t n)
 {
     return memmove_c_forward_64(dst, d, s, n);
 }
 
-static void *__cdecl memmove_c_backward(void *dst, char *d, const char *s, size_t n)
+static void *memmove_c_backward(void *dst, char *d, const char *s, size_t n)
 {
     return memmove_c_backward_64(dst, d, s, n);
 }
 
-static void *__cdecl memmove_c(char *d, const char *s, size_t n)
+static void *memmove_c(char *d, const char *s, size_t n)
 {
     if (__builtin_expect(n <= 32, 1)) return memmove_c_unaligned_32(d, s, n);
     if (d < s) return memmove_c_forward(d, d, s, n);
     else return memmove_c_backward(d, d + n, s + n, n);
 }
 
-static void *__cdecl memset_c(char *d, int v, size_t n)
+static void *memset_c(char *d, int v, size_t n)
 {
     if (__builtin_expect(n <= 32, 1)) return memset_c_unaligned_32(d, v, n);
     return memset_c_64(d, d, v, n);
@@ -2706,7 +2710,7 @@ static void *__cdecl memset_c(char *d, int v, size_t n)
 #define __DISABLE_SSE2__
 #endif /* __SSE2__ */
 
-static inline void *__cdecl memmove_sse2_unaligned_64(char *d, const char *s, size_t n)
+static inline void *memmove_sse2_unaligned_64(char *d, const char *s, size_t n)
 {
     if (n > 32)
     {
@@ -2760,13 +2764,13 @@ static inline void *__cdecl memmove_sse2_unaligned_64(char *d, const char *s, si
     return d;
 }
 
-static inline void *__cdecl memmove_sse2_forward_32(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_sse2_forward_32(void *dst, char *d, const char *s, size_t n)
 {
     memmove_sse2_unaligned_64(d, s, n);
     return dst;
 }
 
-static inline void *__cdecl memmove_sse2_backward_32(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_sse2_backward_32(void *dst, char *d, const char *s, size_t n)
 {
     return memmove_sse2_unaligned_64(d - n, s - n, n);
 }
@@ -2808,13 +2812,13 @@ MEMMOVE_DECLARE(sse2, 16,  256, 128,
     _mm_store_si128((__m128i*)(d +240 * z + y), _mm_loadu_si128((__m128i *)(s +240 * z + y)));
 )
 
-static inline void *__cdecl memmove_aligned_sse2_forward_32(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_aligned_sse2_forward_32(void *dst, char *d, const char *s, size_t n)
 {
     memmove_sse2_unaligned_64(d, s, n);
     return dst;
 }
 
-static inline void *__cdecl memmove_aligned_sse2_backward_32(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_aligned_sse2_backward_32(void *dst, char *d, const char *s, size_t n)
 {
     return memmove_sse2_unaligned_64(d - n, s - n, n);
 }
@@ -2856,7 +2860,7 @@ MEMMOVE_DECLARE(aligned_sse2, 16,  256, 128,
     _mm_store_si128((__m128i*)(d +240 * z + y), _mm_load_si128((__m128i *)(s +240 * z + y)));
 )
 
-static inline void *__cdecl memset_sse2_unaligned_64(char *d, int v, size_t n)
+static inline void *memset_sse2_unaligned_64(char *d, int v, size_t n)
 {
     uint8_t tmp8 = v;
     uint16_t tmp16 = ((uint16_t)tmp8 << 8) | tmp8;
@@ -2905,7 +2909,7 @@ static inline void *__cdecl memset_sse2_unaligned_64(char *d, int v, size_t n)
     return d;
 }
 
-static inline void *__cdecl memset_sse2_32(void *dst, char *d, int v, size_t n)
+static inline void *memset_sse2_32(void *dst, char *d, int v, size_t n)
 {
     memset_sse2_unaligned_64(d, v, n);
     return dst;
@@ -2932,35 +2936,35 @@ MEMSET_DECLARE(sse2, 128, 64, __m128i tmp128 = _mm_set1_epi64x(tmp64),
 #define MEMMOVE_SSE2_FORWARD_MISALIGN(x) (0x10 - ((uintptr_t)x & 0xf))
 #define MEMMOVE_SSE2_BACKWARD_MISALIGN(x) ((uintptr_t)x & 0xf)
 
-static void *__cdecl memmove_sse2_forward(void *dst, char *d, const char *s, size_t n)
+static void *memmove_sse2_forward(void *dst, char *d, const char *s, size_t n)
 {
     size_t k = MEMMOVE_SSE2_FORWARD_MISALIGN(d);
     memmove_sse2_unaligned_64(d, s, k);
     d += k; s += k; n -= k;
-    if (MEMMOVE_SSE2_FORWARD_MISALIGN(s)) return memmove_sse2_forward_256(dst, d, s, n);
+    if (unlikely(MEMMOVE_SSE2_FORWARD_MISALIGN(s))) return memmove_sse2_forward_256(dst, d, s, n);
     else return memmove_aligned_sse2_forward_256(dst, d, s, n);
 }
 
-static void *__cdecl memmove_sse2_backward(void *dst, char *d, const char *s, size_t n)
+static void *memmove_sse2_backward(void *dst, char *d, const char *s, size_t n)
 {
     size_t k = MEMMOVE_SSE2_BACKWARD_MISALIGN(d);
     d -= k; s -= k; n -= k;
     memmove_sse2_unaligned_64(d, s, k);
-    if (MEMMOVE_SSE2_BACKWARD_MISALIGN(s)) return memmove_sse2_backward_256(dst, d, s, n);
+    if (unlikely(MEMMOVE_SSE2_BACKWARD_MISALIGN(s))) return memmove_sse2_backward_256(dst, d, s, n);
     else return memmove_aligned_sse2_backward_256(dst, d, s, n);
 }
 
-static void *__cdecl memmove_sse2(char *d, const char *s, size_t n)
+void *memmove_sse2(char *d, const char *s, size_t n)
 {
-    if (__builtin_expect(n <= 64, 1)) return memmove_sse2_unaligned_64(d, s, n);
+    if (unlikely(n <= 64)) return memmove_sse2_unaligned_64(d, s, n);
     else if (d < s) return memmove_sse2_forward(d, d, s, n);
     else return memmove_sse2_backward(d, d + n, s + n, n);
 }
 
-static void *__cdecl memset_sse2(char *d, int v, size_t n)
+void *memset_sse2(char *d, int v, size_t n)
 {
     size_t k = MEMMOVE_SSE2_FORWARD_MISALIGN(d);
-    if (__builtin_expect(n <= 64, 1)) return memset_sse2_unaligned_64(d, v, n);
+    if (unlikely(n <= 64)) return memset_sse2_unaligned_64(d, v, n);
     memset_sse2_unaligned_64(d, v, k);
     return memset_sse2_128(d, d + k, v, n - k);
 }
@@ -2987,7 +2991,7 @@ static void *__cdecl memset_sse2(char *d, int v, size_t n)
 #define __DISABLE_AVX__
 #endif /* __AVX__ */
 
-static inline void *__cdecl memmove_avx_unaligned_128(char *d, const char *s, size_t n)
+static inline void *memmove_avx_unaligned_128(char *d, const char *s, size_t n)
 {
     if (n > 64)
     {
@@ -3049,103 +3053,159 @@ static inline void *__cdecl memmove_avx_unaligned_128(char *d, const char *s, si
     return d;
 }
 
-static inline void *__cdecl memmove_avx_forward_64(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_avx_forward_64(void *dst, char *d, const char *s, size_t n)
 {
     memmove_avx_unaligned_128(d, s, n);
     return dst;
 }
 
-static inline void *__cdecl memmove_avx_backward_64(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_avx_backward_64(void *dst, char *d, const char *s, size_t n)
 {
     return memmove_avx_unaligned_128(d - n, s - n, n);
 }
 
 MEMMOVE_DECLARE(avx, 32, 128, 64,
-    _mm256_store_si256((__m256i*)(d +  0 * z + y), _mm256_loadu_si256((__m256i *)(s +  0 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 32 * z + y), _mm256_loadu_si256((__m256i *)(s + 32 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 64 * z + y), _mm256_loadu_si256((__m256i *)(s + 64 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 96 * z + y), _mm256_loadu_si256((__m256i *)(s + 96 * z + y)));
+    __m256i ymm0 = _mm256_loadu_si256((__m256i *)(s +  0 * z + y));
+    __m256i ymm1 = _mm256_loadu_si256((__m256i *)(s + 32 * z + y));
+    __m256i ymm2 = _mm256_loadu_si256((__m256i *)(s + 64 * z + y));
+    __m256i ymm3 = _mm256_loadu_si256((__m256i *)(s + 96 * z + y));
+    _mm256_store_si256((__m256i*)(d +  0 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d + 32 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d + 64 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d + 96 * z + y), ymm3);
 )
 
 MEMMOVE_DECLARE(avx, 32, 256, 128,
-    _mm256_store_si256((__m256i*)(d +  0 * z + y), _mm256_loadu_si256((__m256i *)(s +  0 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 32 * z + y), _mm256_loadu_si256((__m256i *)(s + 32 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 64 * z + y), _mm256_loadu_si256((__m256i *)(s + 64 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 96 * z + y), _mm256_loadu_si256((__m256i *)(s + 96 * z + y)));
-    _mm256_store_si256((__m256i*)(d +128 * z + y), _mm256_loadu_si256((__m256i *)(s +128 * z + y)));
-    _mm256_store_si256((__m256i*)(d +160 * z + y), _mm256_loadu_si256((__m256i *)(s +160 * z + y)));
-    _mm256_store_si256((__m256i*)(d +192 * z + y), _mm256_loadu_si256((__m256i *)(s +192 * z + y)));
-    _mm256_store_si256((__m256i*)(d +224 * z + y), _mm256_loadu_si256((__m256i *)(s +224 * z + y)));
+    __m256i ymm0 = _mm256_loadu_si256((__m256i *)(s +  0 * z + y));
+    __m256i ymm1 = _mm256_loadu_si256((__m256i *)(s + 32 * z + y));
+    __m256i ymm2 = _mm256_loadu_si256((__m256i *)(s + 64 * z + y));
+    __m256i ymm3 = _mm256_loadu_si256((__m256i *)(s + 96 * z + y));
+    _mm256_store_si256((__m256i*)(d +  0 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d + 32 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d + 64 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d + 96 * z + y), ymm3);
+    ymm0 = _mm256_loadu_si256((__m256i *)(s +128 * z + y));
+    ymm1 = _mm256_loadu_si256((__m256i *)(s +160 * z + y));
+    ymm2 = _mm256_loadu_si256((__m256i *)(s +192 * z + y));
+    ymm3 = _mm256_loadu_si256((__m256i *)(s +224 * z + y));
+    _mm256_store_si256((__m256i*)(d +128 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d +160 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d +192 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d +224 * z + y), ymm3);
 )
 
 MEMMOVE_DECLARE(avx, 32, 512, 256,
-    _mm256_store_si256((__m256i*)(d +  0 * z + y), _mm256_loadu_si256((__m256i *)(s +  0 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 32 * z + y), _mm256_loadu_si256((__m256i *)(s + 32 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 64 * z + y), _mm256_loadu_si256((__m256i *)(s + 64 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 96 * z + y), _mm256_loadu_si256((__m256i *)(s + 96 * z + y)));
-    _mm256_store_si256((__m256i*)(d +128 * z + y), _mm256_loadu_si256((__m256i *)(s +128 * z + y)));
-    _mm256_store_si256((__m256i*)(d +160 * z + y), _mm256_loadu_si256((__m256i *)(s +160 * z + y)));
-    _mm256_store_si256((__m256i*)(d +192 * z + y), _mm256_loadu_si256((__m256i *)(s +192 * z + y)));
-    _mm256_store_si256((__m256i*)(d +224 * z + y), _mm256_loadu_si256((__m256i *)(s +224 * z + y)));
-    _mm256_store_si256((__m256i*)(d +256 * z + y), _mm256_loadu_si256((__m256i *)(s +256 * z + y)));
-    _mm256_store_si256((__m256i*)(d +288 * z + y), _mm256_loadu_si256((__m256i *)(s +288 * z + y)));
-    _mm256_store_si256((__m256i*)(d +320 * z + y), _mm256_loadu_si256((__m256i *)(s +320 * z + y)));
-    _mm256_store_si256((__m256i*)(d +352 * z + y), _mm256_loadu_si256((__m256i *)(s +352 * z + y)));
-    _mm256_store_si256((__m256i*)(d +384 * z + y), _mm256_loadu_si256((__m256i *)(s +384 * z + y)));
-    _mm256_store_si256((__m256i*)(d +416 * z + y), _mm256_loadu_si256((__m256i *)(s +416 * z + y)));
-    _mm256_store_si256((__m256i*)(d +448 * z + y), _mm256_loadu_si256((__m256i *)(s +448 * z + y)));
-    _mm256_store_si256((__m256i*)(d +480 * z + y), _mm256_loadu_si256((__m256i *)(s +480 * z + y)));
+    __m256i ymm0 = _mm256_loadu_si256((__m256i *)(s +  0 * z + y));
+    __m256i ymm1 = _mm256_loadu_si256((__m256i *)(s + 32 * z + y));
+    __m256i ymm2 = _mm256_loadu_si256((__m256i *)(s + 64 * z + y));
+    __m256i ymm3 = _mm256_loadu_si256((__m256i *)(s + 96 * z + y));
+    _mm256_store_si256((__m256i*)(d +  0 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d + 32 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d + 64 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d + 96 * z + y), ymm3);
+    ymm0 = _mm256_loadu_si256((__m256i *)(s +128 * z + y));
+    ymm1 = _mm256_loadu_si256((__m256i *)(s +160 * z + y));
+    ymm2 = _mm256_loadu_si256((__m256i *)(s +192 * z + y));
+    ymm3 = _mm256_loadu_si256((__m256i *)(s +224 * z + y));
+    _mm256_store_si256((__m256i*)(d +128 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d +160 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d +192 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d +224 * z + y), ymm3);
+    ymm0 = _mm256_loadu_si256((__m256i *)(s +256 * z + y));
+    ymm1 = _mm256_loadu_si256((__m256i *)(s +288 * z + y));
+    ymm2 = _mm256_loadu_si256((__m256i *)(s +320 * z + y));
+    ymm3 = _mm256_loadu_si256((__m256i *)(s +352 * z + y));
+    _mm256_store_si256((__m256i*)(d +256 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d +288 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d +320 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d +352 * z + y), ymm3);
+    ymm0 = _mm256_loadu_si256((__m256i *)(s +384 * z + y));
+    ymm1 = _mm256_loadu_si256((__m256i *)(s +416 * z + y));
+    ymm2 = _mm256_loadu_si256((__m256i *)(s +448 * z + y));
+    ymm3 = _mm256_loadu_si256((__m256i *)(s +480 * z + y));
+    _mm256_store_si256((__m256i*)(d +384 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d +416 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d +448 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d +480 * z + y), ymm3);
 )
 
-static inline void *__cdecl memmove_aligned_avx_forward_64(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_aligned_avx_forward_64(void *dst, char *d, const char *s, size_t n)
 {
     memmove_avx_unaligned_128(d, s, n);
     return dst;
 }
 
-static inline void *__cdecl memmove_aligned_avx_backward_64(void *dst, char *d, const char *s, size_t n)
+static inline void *memmove_aligned_avx_backward_64(void *dst, char *d, const char *s, size_t n)
 {
     return memmove_avx_unaligned_128(d - n, s - n, n);
 }
 
 MEMMOVE_DECLARE(aligned_avx, 32, 128, 64,
-    _mm256_store_si256((__m256i*)(d +  0 * z + y), _mm256_load_si256((__m256i *)(s +  0 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 32 * z + y), _mm256_load_si256((__m256i *)(s + 32 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 64 * z + y), _mm256_load_si256((__m256i *)(s + 64 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 96 * z + y), _mm256_load_si256((__m256i *)(s + 96 * z + y)));
+    __m256i ymm0 = _mm256_load_si256((__m256i *)(s +  0 * z + y));
+    __m256i ymm1 = _mm256_load_si256((__m256i *)(s + 32 * z + y));
+    __m256i ymm2 = _mm256_load_si256((__m256i *)(s + 64 * z + y));
+    __m256i ymm3 = _mm256_load_si256((__m256i *)(s + 96 * z + y));
+    _mm256_store_si256((__m256i*)(d +  0 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d + 32 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d + 64 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d + 96 * z + y), ymm3);
 )
 
 MEMMOVE_DECLARE(aligned_avx, 32, 256, 128,
-    _mm256_store_si256((__m256i*)(d +  0 * z + y), _mm256_load_si256((__m256i *)(s +  0 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 32 * z + y), _mm256_load_si256((__m256i *)(s + 32 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 64 * z + y), _mm256_load_si256((__m256i *)(s + 64 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 96 * z + y), _mm256_load_si256((__m256i *)(s + 96 * z + y)));
-    _mm256_store_si256((__m256i*)(d +128 * z + y), _mm256_load_si256((__m256i *)(s +128 * z + y)));
-    _mm256_store_si256((__m256i*)(d +160 * z + y), _mm256_load_si256((__m256i *)(s +160 * z + y)));
-    _mm256_store_si256((__m256i*)(d +192 * z + y), _mm256_load_si256((__m256i *)(s +192 * z + y)));
-    _mm256_store_si256((__m256i*)(d +224 * z + y), _mm256_load_si256((__m256i *)(s +224 * z + y)));
+    __m256i ymm0 = _mm256_load_si256((__m256i *)(s +  0 * z + y));
+    __m256i ymm1 = _mm256_load_si256((__m256i *)(s + 32 * z + y));
+    __m256i ymm2 = _mm256_load_si256((__m256i *)(s + 64 * z + y));
+    __m256i ymm3 = _mm256_load_si256((__m256i *)(s + 96 * z + y));
+    _mm256_store_si256((__m256i*)(d +  0 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d + 32 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d + 64 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d + 96 * z + y), ymm3);
+    ymm0 = _mm256_load_si256((__m256i *)(s +128 * z + y));
+    ymm1 = _mm256_load_si256((__m256i *)(s +160 * z + y));
+    ymm2 = _mm256_load_si256((__m256i *)(s +192 * z + y));
+    ymm3 = _mm256_load_si256((__m256i *)(s +224 * z + y));
+    _mm256_store_si256((__m256i*)(d +128 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d +160 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d +192 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d +224 * z + y), ymm3);
 )
 
 MEMMOVE_DECLARE(aligned_avx, 32, 512, 256,
-    _mm256_store_si256((__m256i*)(d +  0 * z + y), _mm256_load_si256((__m256i *)(s +  0 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 32 * z + y), _mm256_load_si256((__m256i *)(s + 32 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 64 * z + y), _mm256_load_si256((__m256i *)(s + 64 * z + y)));
-    _mm256_store_si256((__m256i*)(d + 96 * z + y), _mm256_load_si256((__m256i *)(s + 96 * z + y)));
-    _mm256_store_si256((__m256i*)(d +128 * z + y), _mm256_load_si256((__m256i *)(s +128 * z + y)));
-    _mm256_store_si256((__m256i*)(d +160 * z + y), _mm256_load_si256((__m256i *)(s +160 * z + y)));
-    _mm256_store_si256((__m256i*)(d +192 * z + y), _mm256_load_si256((__m256i *)(s +192 * z + y)));
-    _mm256_store_si256((__m256i*)(d +224 * z + y), _mm256_load_si256((__m256i *)(s +224 * z + y)));
-    _mm256_store_si256((__m256i*)(d +256 * z + y), _mm256_load_si256((__m256i *)(s +256 * z + y)));
-    _mm256_store_si256((__m256i*)(d +288 * z + y), _mm256_load_si256((__m256i *)(s +288 * z + y)));
-    _mm256_store_si256((__m256i*)(d +320 * z + y), _mm256_load_si256((__m256i *)(s +320 * z + y)));
-    _mm256_store_si256((__m256i*)(d +352 * z + y), _mm256_load_si256((__m256i *)(s +352 * z + y)));
-    _mm256_store_si256((__m256i*)(d +384 * z + y), _mm256_load_si256((__m256i *)(s +384 * z + y)));
-    _mm256_store_si256((__m256i*)(d +416 * z + y), _mm256_load_si256((__m256i *)(s +416 * z + y)));
-    _mm256_store_si256((__m256i*)(d +448 * z + y), _mm256_load_si256((__m256i *)(s +448 * z + y)));
-    _mm256_store_si256((__m256i*)(d +480 * z + y), _mm256_load_si256((__m256i *)(s +480 * z + y)));
+    __m256i ymm0 = _mm256_load_si256((__m256i *)(s +  0 * z + y));
+    __m256i ymm1 = _mm256_load_si256((__m256i *)(s + 32 * z + y));
+    __m256i ymm2 = _mm256_load_si256((__m256i *)(s + 64 * z + y));
+    __m256i ymm3 = _mm256_load_si256((__m256i *)(s + 96 * z + y));
+    _mm256_store_si256((__m256i*)(d +  0 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d + 32 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d + 64 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d + 96 * z + y), ymm3);
+    ymm0 = _mm256_load_si256((__m256i *)(s +128 * z + y));
+    ymm1 = _mm256_load_si256((__m256i *)(s +160 * z + y));
+    ymm2 = _mm256_load_si256((__m256i *)(s +192 * z + y));
+    ymm3 = _mm256_load_si256((__m256i *)(s +224 * z + y));
+    _mm256_store_si256((__m256i*)(d +128 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d +160 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d +192 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d +224 * z + y), ymm3);
+    ymm0 = _mm256_load_si256((__m256i *)(s +256 * z + y));
+    ymm1 = _mm256_load_si256((__m256i *)(s +288 * z + y));
+    ymm2 = _mm256_load_si256((__m256i *)(s +320 * z + y));
+    ymm3 = _mm256_load_si256((__m256i *)(s +352 * z + y));
+    _mm256_store_si256((__m256i*)(d +256 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d +288 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d +320 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d +352 * z + y), ymm3);
+    ymm0 = _mm256_load_si256((__m256i *)(s +384 * z + y));
+    ymm1 = _mm256_load_si256((__m256i *)(s +416 * z + y));
+    ymm2 = _mm256_load_si256((__m256i *)(s +448 * z + y));
+    ymm3 = _mm256_load_si256((__m256i *)(s +480 * z + y));
+    _mm256_store_si256((__m256i*)(d +384 * z + y), ymm0);
+    _mm256_store_si256((__m256i*)(d +416 * z + y), ymm1);
+    _mm256_store_si256((__m256i*)(d +448 * z + y), ymm2);
+    _mm256_store_si256((__m256i*)(d +480 * z + y), ymm3);
 )
 
-static inline void *__cdecl memset_avx_unaligned_128(char *d, int v, size_t n)
+static inline void *memset_avx_unaligned_128(char *d, int v, size_t n)
 {
     uint8_t tmp8 = v;
     uint16_t tmp16 = ((uint16_t)tmp8 << 8) | tmp8;
@@ -3201,7 +3261,7 @@ static inline void *__cdecl memset_avx_unaligned_128(char *d, int v, size_t n)
     return d;
 }
 
-static inline void *__cdecl memset_avx_64(void *dst, char *d, int v, size_t n)
+static inline void *memset_avx_64(void *dst, char *d, int v, size_t n)
 {
     memset_avx_unaligned_128(d, v, n);
     return dst;
@@ -3247,35 +3307,35 @@ MEMSET_DECLARE(avx, 512, 256, __m256i tmp256 = _mm256_set1_epi64x(tmp64),
 #define MEMMOVE_AVX_FORWARD_MISALIGN(x) (0x20 - ((uintptr_t)x & 0x1f))
 #define MEMMOVE_AVX_BACKWARD_MISALIGN(x) ((uintptr_t)x & 0x1f)
 
-static void *__cdecl memmove_avx_forward(void *dst, char *d, const char *s, size_t n)
+static void *memmove_avx_forward(void *dst, char *d, const char *s, size_t n)
 {
     size_t k = MEMMOVE_AVX_FORWARD_MISALIGN(d);
     memmove_avx_unaligned_128(d, s, k);
     d += k; s += k; n -= k;
-    if (MEMMOVE_AVX_FORWARD_MISALIGN(s)) return memmove_avx_forward_512(dst, d, s, n);
+    if (unlikely(MEMMOVE_AVX_FORWARD_MISALIGN(s))) return memmove_avx_forward_512(dst, d, s, n);
     else return memmove_aligned_avx_forward_512(dst, d, s, n);
 }
 
-static void *__cdecl memmove_avx_backward(void *dst, char *d, const char *s, size_t n)
+static void *memmove_avx_backward(void *dst, char *d, const char *s, size_t n)
 {
     size_t k = MEMMOVE_AVX_BACKWARD_MISALIGN(d);
     d -= k; s -= k; n -= k;
     memmove_avx_unaligned_128(d, s, k);
-    if (MEMMOVE_AVX_BACKWARD_MISALIGN(s)) return memmove_avx_backward_512(dst, d, s, n);
+    if (unlikely(MEMMOVE_AVX_BACKWARD_MISALIGN(s))) return memmove_avx_backward_512(dst, d, s, n);
     else return memmove_aligned_avx_backward_512(dst, d, s, n);
 }
 
-static void *__cdecl memmove_avx(char *d, const char *s, size_t n)
+void *memmove_avx(char *d, const char *s, size_t n)
 {
-    if (__builtin_expect(n <= 128, 1)) return memmove_avx_unaligned_128(d, s, n);
+    if (unlikely(n <= 128)) return memmove_avx_unaligned_128(d, s, n);
     else if (d < s) return memmove_avx_forward(d, d, s, n);
     else return memmove_avx_backward(d, d + n, s + n, n);
 }
 
-static void *__cdecl memset_avx(char *d, int v, size_t n)
+void *memset_avx(char *d, int v, size_t n)
 {
     size_t k = MEMMOVE_AVX_FORWARD_MISALIGN(d);
-    if (__builtin_expect(n <= 128, 1)) return memset_avx_unaligned_128(d, v, n);
+    if (unlikely(n <= 128)) return memset_avx_unaligned_128(d, v, n);
     memset_avx_unaligned_128(d, v, k);
     return memset_avx_512(d, d + k, v, n - k);
 }
@@ -3305,36 +3365,39 @@ static void *__cdecl memset_avx(char *d, int v, size_t n)
 /*********************************************************************
  *                  memmove (MSVCRT.@)
  */
-void * __cdecl memmove(void *dst, const void *src, size_t n)
+void *__cdecl memmove(void *dst, const void *src, size_t n)
 {
-    if (__builtin_expect(n <= 32, 1)) return memmove_c_unaligned_32(dst, src, n);
-    if (__builtin_expect(avx_supported, 1)) return memmove_avx(dst, src, n);
-    if (__builtin_expect(sse2_supported, 1)) return memmove_sse2(dst, src, n);
+    if (unlikely(n <= 32)) return memmove_c_unaligned_32(dst, src, n);
+    if (likely(avx_supported)) return memmove_avx(dst, src, n);
+    if (likely(sse2_supported)) return memmove_sse2(dst, src, n);
     return memmove_c(dst, src, n);
 }
 
 /*********************************************************************
  *                  memcpy   (MSVCRT.@)
  */
-void * __cdecl memcpy(void *dst, const void *src, size_t n)
+void *__cdecl memcpy(void *dst, const void *src, size_t n)
 {
-    if (__builtin_expect(n <= 32, 1)) return memmove_c_unaligned_32(dst, src, n);
-    if (__builtin_expect(avx_supported, 1)) return memmove_avx(dst, src, n);
-    if (__builtin_expect(sse2_supported, 1)) return memmove_sse2(dst, src, n);
+    if (unlikely(n <= 32)) return memmove_c_unaligned_32(dst, src, n);
+    if (likely(avx_supported)) return memmove_avx(dst, src, n);
+    if (likely(sse2_supported)) return memmove_sse2(dst, src, n);
     return memmove_c(dst, src, n);
 }
 
 /*********************************************************************
  *		    memset (MSVCRT.@)
  */
-void* __cdecl memset(void *dst, int c, size_t n)
+void *__cdecl memset(void *dst, int c, size_t n)
 {
-    if (__builtin_expect(n <= 32, 1)) return memset_c_unaligned_32(dst, c, n);
-    if (__builtin_expect(avx_supported, 1)) return memset_avx(dst, c, n);
-    if (__builtin_expect(sse2_supported, 1)) return memset_sse2(dst, c, n);
+    if (unlikely(n <= 32)) return memset_c_unaligned_32(dst, c, n);
+    if (likely(avx_supported)) return memset_avx(dst, c, n);
+    if (likely(sse2_supported)) return memset_sse2(dst, c, n);
     return memset_c(dst, c, n);
 }
 
+#undef likely
+#undef unlikely
+
 /*********************************************************************
  *		    strchr (MSVCRT.@)
  */
-- 
2.30.2

